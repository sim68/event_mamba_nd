{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f00903-648c-4622-acf6-d2ad104799d1",
   "metadata": {},
   "source": [
    "# IN PROGRESS... No RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7efd2ca6-aaca-47c9-ada8-a2ab29ab2526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version PyTorch : 2.1.2\n",
      "Version CUDA utilisée par PyTorch : 11.8\n",
      "True\n",
      "8700\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Version PyTorch :\", torch.__version__)\n",
    "print(\"Version CUDA utilisée par PyTorch :\", torch.version.cuda)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce614dd-bea7-4c72-8dd0-e9a1a99a5dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "<torch.cuda.device object at 0x7ffbd85df100>\n",
      "Quadro RTX 5000\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676035d7-75c3-4356-8e0e-f4ba31d5c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mim install mmcv==2.1.0\n",
    "# pip install mmaction2\n",
    "# pip install timm\n",
    "# pip install pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc662c3-7485-4fc6-bf4c-0d1fd586d86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([2, 64, 16])\n",
      "output torch.Size([2, 64, 16])\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "############ TEST MAMBA LAYER  ###########\n",
    "##########################################\n",
    "\n",
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape\n",
    "print(\"input\", x.shape)\n",
    "print(\"output\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59485ad-186d-4308-a131-e039fe31e9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import mmcv\n",
    "print(mmcv.__version__)  # 2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a758631-1887-4ce1-a4b4-60b8562aba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import mmaction\n",
    "print(mmaction.__version__)  # 1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abb157e-263b-4e5d-a9fd-b3776079ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "from typing import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mmcv.cnn.bricks.transformer import FFN, PatchEmbed\n",
    "from mmengine.model import BaseModule, ModuleList\n",
    "from mmengine.model.weight_init import trunc_normal_\n",
    "from mmcv.cnn import build_norm_layer\n",
    "from mmengine.utils import to_2tuple\n",
    "from typing import Tuple\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "MambaND = nn.Identity # TODO: cleanup and release\n",
    "# from ..utils import (MultiheadAttention, SwiGLUFFNFused, build_norm_layer,\n",
    "#                      resize_pos_embed, to_2tuple)\n",
    "\n",
    "def resize_pos_embed(pos_embed: torch.Tensor,\n",
    "                     src_shape: Tuple[int],\n",
    "                     dst_shape: Tuple[int],\n",
    "                     mode = 'trilinear',\n",
    "                     num_extra_tokens: int = 1) -> torch.Tensor:\n",
    "    \"\"\"Resize pos_embed weights.\n",
    "\n",
    "    Args:\n",
    "        pos_embed (torch.Tensor): Position embedding weights with shape\n",
    "            [1, L, C].\n",
    "        src_shape (tuple): The resolution of downsampled origin training\n",
    "            image, in format (T, H, W).\n",
    "        dst_shape (tuple): The resolution of downsampled new training\n",
    "            image, in format (T, H, W).\n",
    "        mode (str): Algorithm used for upsampling. Choose one from 'nearest',\n",
    "            'linear', 'bilinear', 'bicubic' and 'trilinear'.\n",
    "            Defaults to 'trilinear'.\n",
    "        num_extra_tokens (int): The number of extra tokens, such as cls_token.\n",
    "            Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The resized pos_embed of shape [1, L_new, C]\n",
    "    \"\"\"\n",
    "    if src_shape[0] == dst_shape[0] and src_shape[1] == dst_shape[1] \\\n",
    "            and src_shape[2] == dst_shape[2]:\n",
    "        return pos_embed\n",
    "    assert pos_embed.ndim == 3, 'shape of pos_embed must be [1, L, C]'\n",
    "    _, L, C = pos_embed.shape\n",
    "    src_t, src_h, src_w = src_shape\n",
    "    assert L == src_t * src_h * src_w + num_extra_tokens, \\\n",
    "        f\"The length of `pos_embed` ({L}) doesn't match the expected \" \\\n",
    "        f'shape ({src_t}*{src_h}*{src_w}+{num_extra_tokens}).' \\\n",
    "        'Please check the `img_size` argument.'\n",
    "    extra_tokens = pos_embed[:, :num_extra_tokens]\n",
    "\n",
    "    src_weight = pos_embed[:, num_extra_tokens:]\n",
    "    src_weight = src_weight.reshape(1, src_t, src_h, src_w,\n",
    "                                    C).permute(0, 4, 1, 2, 3)\n",
    "\n",
    "    dst_weight = F.interpolate(\n",
    "        src_weight, size=dst_shape, align_corners=False, mode=mode)\n",
    "    dst_weight = torch.flatten(dst_weight, 2).transpose(1, 2)\n",
    "\n",
    "    return torch.cat((extra_tokens, dst_weight), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# from .base_backbone import BaseBackbone\n",
    "from mamba_ssm import Mamba\n",
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
    "from functools import partial\n",
    "#from .ssm2d import Block2D,Mamba2D,SplitHead2D\n",
    "Block2D = Mamba2D = SplitHead2D = nn.Identity\n",
    "from einops import rearrange\n",
    "from mmengine.logging import MMLogger\n",
    "\n",
    "from mmcv.cnn.bricks.drop import build_dropout\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False,reverse=False,\n",
    "        transpose=False,split_head=False,\n",
    "        drop_path_rate=0.0,drop_rate=0.0,use_mlp=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n",
    "\n",
    "        This Block has a slightly different structure compared to a regular\n",
    "        prenorm Transformer block.\n",
    "        The standard block is: LN -> MHA/MLP -> Add.\n",
    "        [Ref: https://arxiv.org/abs/2002.04745]\n",
    "        Here we have: Add -> LN -> Mixer, returning both\n",
    "        the hidden_states (output of the mixer) and the residual.\n",
    "        This is purely for performance reasons, as we can fuse add and LayerNorm.\n",
    "        The residual needs to be provided (except for the very first block).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        self.fused_add_norm = fused_add_norm\n",
    "        self.mixer = mixer_cls(dim)\n",
    "        self.norm = norm_cls(dim)\n",
    "        self.split_head = split_head\n",
    "        self.reverse = reverse\n",
    "        self.transpose = transpose\n",
    "        self.drop_path = build_dropout(\n",
    "            dict(type='DropPath', drop_prob=drop_path_rate)\n",
    "        )\n",
    "        self.dropout = build_dropout(\n",
    "            dict(type='Dropout', drop_prob=drop_rate)\n",
    "        )\n",
    "        if use_mlp:\n",
    "            self.ffn = SwiGLUFFNFused(\n",
    "                    embed_dims=dim,\n",
    "                    feedforward_channels=int(dim*4),\n",
    "                    layer_scale_init_value=0.0)\n",
    "            self.ln2 = build_norm_layer(dict(type='LN'), dim)\n",
    "        else:\n",
    "            self.ffn = None\n",
    "        if self.fused_add_norm:\n",
    "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
    "            assert isinstance(\n",
    "                self.norm, (nn.LayerNorm, RMSNorm)\n",
    "            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None,order='t l h w',\n",
    "        shape=None,skip=True,n_dim_pos=4\n",
    "    ):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: the sequence to the encoder layer (required).\n",
    "            residual: hidden_states = Mixer(LN(residual))\n",
    "        \"\"\"\n",
    "        h = w = 0\n",
    "        assert shape is not None\n",
    "        t,l,h,w = shape\n",
    "        if n_dim_pos != 4:\n",
    "            order = order.split(' ')\n",
    "            assert len(order) == 4\n",
    "            trunc_n = 4 - n_dim_pos\n",
    "            tgt_order = f\"(n {' '.join(order[:trunc_n])}) ({' '.join(order[trunc_n:])}) c\"\n",
    "        else:\n",
    "            tgt_order = f'n ({order}) c'\n",
    "        hidden_states =  rearrange(hidden_states,f'n (t l h w ) c -> {tgt_order}',t=t,l=l,h=h,w=w)\n",
    "\n",
    "        # if self.transpose:\n",
    "        #     l = hidden_states.shape[1]\n",
    "        #     h = w = int(np.sqrt(l))\n",
    "        #     # assert h * w == l\n",
    "        #     hidden_states = rearrange(hidden_states,'n (h w) c -> n (w h) c',h=h,w=w)\n",
    "        #     if residual is not None:\n",
    "        #         residual = rearrange(residual,'n (h w) c -> n (w h) c',h=h,w=w)\n",
    "        if self.reverse:\n",
    "            hidden_states = hidden_states.flip(1)\n",
    "            # print(\"reverse\")\n",
    "            if residual is not None:\n",
    "                residual = residual.flip(1)\n",
    "        if not self.fused_add_norm:\n",
    "            hidden_states = self.norm(hidden_states)\n",
    "            # residual = (hidden_states + residual) if residual is not None else hidden_states\n",
    "            # hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
    "            # if self.residual_in_fp32:\n",
    "            #     residual = residual.to(torch.float32)\n",
    "            if self.split_head:\n",
    "                l = hidden_states.shape[1]\n",
    "                h = w = int(np.sqrt(l))\n",
    "                hidden_states = SplitHead2D.apply(hidden_states,4,h,w)\n",
    "            if skip:\n",
    "                hidden_states = hidden_states + self.drop_path(self.dropout(self.mixer(hidden_states, inference_params=inference_params)))\n",
    "            else:\n",
    "                hidden_states = self.drop_path(self.dropout(self.mixer(hidden_states, inference_params=inference_params)))\n",
    "            if self.split_head:\n",
    "                hidden_states = SplitHead2D.apply(hidden_states,4,h,w)\n",
    "        else:\n",
    "            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n",
    "            hidden_states, residual = fused_add_norm_fn(\n",
    "                hidden_states,\n",
    "                self.norm.weight,\n",
    "                self.norm.bias,\n",
    "                residual=residual,\n",
    "                prenorm=True,\n",
    "                residual_in_fp32=self.residual_in_fp32,\n",
    "                eps=self.norm.eps,\n",
    "            )\n",
    "            hidden_states = self.drop_path(self.mixer(hidden_states, inference_params=inference_params))\n",
    "        if self.ffn is not None:\n",
    "            hidden_states = self.ffn(self.ln2(hidden_states),identity=hidden_states)\n",
    "        if self.reverse:\n",
    "            hidden_states = hidden_states.flip(1)\n",
    "            if residual is not None:\n",
    "                residual = residual.flip(1)\n",
    "        hidden_states =  rearrange(hidden_states,f'{tgt_order}->n (t l h w ) c ',t=t,l=l,h=h,w=w)\n",
    "        # if self.transpose:\n",
    "        #     hidden_states = rearrange(hidden_states,'n (w h) c -> n (h w) c',h=h,w=w)\n",
    "        #     if residual is not None:\n",
    "        #         residual = rearrange(residual,'n (w h) c -> n (h w) c',h=h,w=w)\n",
    "\n",
    "        \n",
    "        # print(\"shape : \", hidden_states.shape)\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
    "\n",
    "def create_block(\n",
    "    d_model,\n",
    "    ssm_cfg=None,\n",
    "    norm_epsilon=1e-5,\n",
    "    rms_norm=False,\n",
    "    residual_in_fp32=False,\n",
    "    fused_add_norm=False,\n",
    "    layer_idx=None,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    "    reverse=None,\n",
    "    is_2d=False,\n",
    "    drop_rate=0.1,\n",
    "    drop_path_rate=0.1,\n",
    "    use_mlp=False,\n",
    "    transpose=False,\n",
    "    split_head=False,\n",
    "    use_nd=False,\n",
    "):\n",
    "    if ssm_cfg is None:\n",
    "        ssm_cfg = {}\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    if use_nd:\n",
    "        transpose = False\n",
    "        reverse = False\n",
    "        mixer_cls = partial(MambaND , layer_idx=layer_idx, n_dim=3,**ssm_cfg, **factory_kwargs)\n",
    "    mixer_cls = partial(Mamba2D if is_2d else Mamba, layer_idx=layer_idx, **ssm_cfg, **factory_kwargs)\n",
    "    norm_cls = partial(\n",
    "        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs\n",
    "    )\n",
    "    if is_2d:\n",
    "        block = Block2D(\n",
    "            d_model,\n",
    "            mixer_cls,\n",
    "            norm_cls=norm_cls,\n",
    "            fused_add_norm=fused_add_norm,\n",
    "            residual_in_fp32=residual_in_fp32,\n",
    "            reverse=reverse,\n",
    "            drop_rate=drop_rate,\n",
    "            transpose=transpose,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "        )\n",
    "    else:\n",
    "        block = Block(\n",
    "            d_model,\n",
    "            mixer_cls,\n",
    "            norm_cls=norm_cls,\n",
    "            fused_add_norm=fused_add_norm,\n",
    "            residual_in_fp32=residual_in_fp32,\n",
    "            reverse=reverse,\n",
    "            transpose=transpose,\n",
    "            drop_rate=drop_rate,\n",
    "            use_mlp=use_mlp,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            split_head=split_head,\n",
    "        )\n",
    "    block.layer_idx = layer_idx\n",
    "    return block \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b24a9-8166-477a-b84e-b6a320ec7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmaction.registry import MODELS\n",
    "from mmaction.utils import ConfigType, OptConfigType\n",
    "from mmengine.runner.checkpoint import _load_checkpoint\n",
    "import re\n",
    "\n",
    "\n",
    "@MODELS.register_module()\n",
    "class Mamba3DModel(BaseModule):\n",
    "    \"\"\"Vision Transformer.\n",
    "\n",
    "    A PyTorch implement of : `An Image is Worth 16x16 Words: Transformers\n",
    "    for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_\n",
    "\n",
    "    Args:\n",
    "        arch (str | dict): Vision Transformer architecture. If use string,\n",
    "            choose from 'small', 'base', 'large', 'deit-tiny', 'deit-small'\n",
    "            and 'deit-base'. If use dict, it should have below keys:\n",
    "\n",
    "            - **embed_dims** (int): The dimensions of embedding.\n",
    "            - **num_layers** (int): The number of transformer encoder layers.\n",
    "            - **num_heads** (int): The number of heads in attention modules.\n",
    "            - **feedforward_channels** (int): The hidden dimensions in\n",
    "              feedforward modules.\n",
    "\n",
    "            Defaults to 'base'.\n",
    "        img_size (int | tuple): The expected input image shape. Because we\n",
    "            support dynamic input shape, just set the argument to the most\n",
    "            common input image shape. Defaults to 224.\n",
    "        patch_size (int | tuple): The patch size in patch embedding.\n",
    "            Defaults to 16.\n",
    "        in_channels (int): The num of input channels. Defaults to 3.\n",
    "        out_indices (Sequence | int): Output from which stages.\n",
    "            Defaults to -1, means the last stage.\n",
    "        drop_rate (float): Probability of an element to be zeroed.\n",
    "            Defaults to 0.\n",
    "        drop_path_rate (float): stochastic depth rate. Defaults to 0.\n",
    "        qkv_bias (bool): Whether to add bias for qkv in attention modules.\n",
    "            Defaults to True.\n",
    "        norm_cfg (dict): Config dict for normalization layer.\n",
    "            Defaults to ``dict(type='LN')``.\n",
    "        final_norm (bool): Whether to add a additional layer to normalize\n",
    "            final feature map. Defaults to True.\n",
    "        out_type (str): The type of output features. Please choose from\n",
    "\n",
    "            - ``\"cls_token\"``: The class token tensor with shape (B, C).\n",
    "            - ``\"featmap\"``: The feature map tensor from the patch tokens\n",
    "              with shape (B, C, H, W).\n",
    "            - ``\"avg_featmap\"``: The global averaged feature map tensor\n",
    "              with shape (B, C).\n",
    "            - ``\"raw\"``: The raw feature tensor includes patch tokens and\n",
    "              class tokens with shape (B, L, C).\n",
    "\n",
    "            Defaults to ``\"cls_token\"``.\n",
    "        with_cls_token (bool): Whether concatenating class token into image\n",
    "            tokens as transformer input. Defaults to True.\n",
    "        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n",
    "            -1 means not freezing any parameters. Defaults to -1.\n",
    "        interpolate_mode (str): Select the interpolate mode for position\n",
    "            embeding vector resize. Defaults to \"bicubic\".\n",
    "        layer_scale_init_value (float or torch.Tensor): Init value of layer\n",
    "            scale. Defaults to 0.\n",
    "        patch_cfg (dict): Configs of patch embeding. Defaults to an empty dict.\n",
    "        layer_cfgs (Sequence | dict): Configs of each transformer layer in\n",
    "            encoder. Defaults to an empty dict.\n",
    "        init_cfg (dict, optional): Initialization config dict.\n",
    "            Defaults to None.\n",
    "    \"\"\"\n",
    "    arch_zoo = {\n",
    "       **dict.fromkeys(\n",
    "            ['small'], {\n",
    "                'embed_dims': 384,\n",
    "                'num_layers': 12,\n",
    "                'num_heads': 6,\n",
    "                'feedforward_channels': 384 * 4\n",
    "            }),\n",
    "   }\n",
    "    num_extra_tokens = 1  # class token\n",
    "    OUT_TYPES = {'raw', 'cls_token', 'featmap', 'avg_featmap'}\n",
    "\n",
    "    def __init__(self,\n",
    "                 arch='base',\n",
    "                 img_size=224,\n",
    "                 patch_size=16,\n",
    "                 patch_size_temporal=2,\n",
    "                 in_channels=3,\n",
    "                 out_indices=-1,\n",
    "                 drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 qkv_bias=True,\n",
    "                 norm_cfg=dict(type='LN', eps=1e-6),\n",
    "                 norm_cfg_2=dict(type='LN', eps=1e-6),\n",
    "                 final_norm=True,\n",
    "                 out_type='featmap',\n",
    "                 with_cls_token=True,\n",
    "                 frozen_stages=-1,\n",
    "                 interpolate_mode='trilinear',\n",
    "                 layer_scale_init_value=0.,\n",
    "                 patch_cfg=dict(),\n",
    "                 layer_cfgs=dict(),\n",
    "                 pre_norm=False,\n",
    "                 init_cfg=None,\n",
    "                 num_frames: int = 16,\n",
    "                 inflate_len=False,\n",
    "                 is_1d=False,\n",
    "                 is_2d=True,\n",
    "                 use_v2=False,\n",
    "                 force_a2=False,\n",
    "                 has_transpose=True,\n",
    "                 fused_add_norm=True,\n",
    "                 use_mlp=False,\n",
    "                 split_head=False,\n",
    "                 pretrained=None,\n",
    "                 pretrained2d=True,\n",
    "                 dt_scale=0.0,\n",
    "                 dt_scale_tmp=0.0,\n",
    "                 use_nd=False,\n",
    "                 force_2d=False,\n",
    "                 update_interval=None,\n",
    "                 copy_weight=False,\n",
    "                 factorization=None,\n",
    "                 inlfate_policy=None,\n",
    "                 n_dim_pos=4,\n",
    "                 d_state=16):\n",
    "        super(Mamba3DModel, self).__init__(init_cfg)\n",
    "        self.force_2d = force_2d\n",
    "        self.use_nd = use_nd\n",
    "        self.inlfate_policy = inlfate_policy\n",
    "        self.pretrained2d = pretrained2d\n",
    "        self.pretrained = pretrained\n",
    "        self.n_dim_pos = n_dim_pos\n",
    "        self.factorization = factorization\n",
    "        self.inflate_len = inflate_len\n",
    "        self.update_interval = update_interval\n",
    "        self.copy_weight = copy_weight\n",
    "        if pretrained:\n",
    "            self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)\n",
    "        if isinstance(arch, str):\n",
    "            arch = arch.lower()\n",
    "            assert arch in set(self.arch_zoo), \\\n",
    "                f'Arch {arch} is not in default archs {set(self.arch_zoo)}'\n",
    "            self.arch_settings = self.arch_zoo[arch]\n",
    "        else:\n",
    "            essential_keys = {\n",
    "                'embed_dims', 'num_layers', 'num_heads', 'feedforward_channels'\n",
    "            }\n",
    "            assert isinstance(arch, dict) and essential_keys <= set(arch), \\\n",
    "                f'Custom arch needs a dict with keys {essential_keys}'\n",
    "            self.arch_settings = arch\n",
    "\n",
    "        self.embed_dims = self.arch_settings['embed_dims']\n",
    "        if self.inflate_len == 4:\n",
    "            self.num_layers = self.arch_settings['num_layers'] * 4\n",
    "        elif self.inflate_len:\n",
    "            self.num_layers = self.arch_settings['num_layers'] * 3\n",
    "        \n",
    "        else:\n",
    "            self.num_layers = self.arch_settings['num_layers'] * (2 if not use_mlp else 1)\n",
    "        self.img_size = (num_frames,img_size,img_size)\n",
    "        self.is_2d = is_2d\n",
    "\n",
    "        # Set patch embedding\n",
    "        _patch_cfg = dict(\n",
    "            in_channels=in_channels,\n",
    "            input_size=self.img_size,\n",
    "            embed_dims=self.embed_dims,\n",
    "            conv_type='Conv3d',\n",
    "            kernel_size=(patch_size_temporal, patch_size, patch_size),\n",
    "            stride=(patch_size_temporal, patch_size, patch_size),\n",
    "            bias=not pre_norm,  # disable bias if pre_norm is used(e.g., CLIP)\n",
    "            padding=(0, 0, 0),\n",
    "            dilation=(1, 1, 1)\n",
    "        )\n",
    "            #         in_channels=in_channels,\n",
    "            # embed_dims=embed_dims,\n",
    "            # conv_type='Conv3d',\n",
    "            # kernel_size=(tubelet_size, patch_size, patch_size),\n",
    "            # stride=(tubelet_size, patch_size, patch_size),\n",
    "            # padding=(0, 0, 0),\n",
    "            # dilation=(1, 1, 1)\n",
    "        _patch_cfg.update(patch_cfg)\n",
    "        self.patch_embed = PatchEmbed(**_patch_cfg)\n",
    "        self.patch_resolution = self.patch_embed.init_out_size\n",
    "        self.patch_resolution = (self.patch_resolution[0],self.patch_resolution[1],self.patch_resolution[1])\n",
    "        num_patches = self.patch_resolution[0] * self.patch_resolution[1]  * self.patch_resolution[1]\n",
    "        self.is_1d = is_1d\n",
    "        # Set out type\n",
    "        if out_type not in self.OUT_TYPES:\n",
    "            raise ValueError(f'Unsupported `out_type` {out_type}, please '\n",
    "                             f'choose from {self.OUT_TYPES}')\n",
    "        self.out_type = out_type\n",
    "\n",
    "        # Set cls token\n",
    "        self.with_cls_token = with_cls_token\n",
    "        if with_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dims))\n",
    "        elif out_type != 'cls_token':\n",
    "            self.cls_token = None\n",
    "            self.num_extra_tokens = 0\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'with_cls_token must be True when `out_type=\"cls_token\"`.')\n",
    "\n",
    "        # Set position embedding\n",
    "        self.interpolate_mode = interpolate_mode\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + self.num_extra_tokens,\n",
    "                        self.embed_dims))\n",
    "        self._register_load_state_dict_pre_hook(self._prepare_pos_embed)\n",
    "\n",
    "        self.drop_after_pos = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        if isinstance(out_indices, int):\n",
    "            out_indices = [out_indices]\n",
    "        assert isinstance(out_indices, Sequence), \\\n",
    "            f'\"out_indices\" must by a sequence or int, ' \\\n",
    "            f'get {type(out_indices)} instead.'\n",
    "        for i, index in enumerate(out_indices):\n",
    "            if index < 0:\n",
    "                out_indices[i] = self.num_layers + index\n",
    "            assert 0 <= out_indices[i] <= self.num_layers, \\\n",
    "                f'Invalid out_indices {index}'\n",
    "        self.out_indices = out_indices\n",
    "\n",
    "        # stochastic depth decay rule\n",
    "        dpr = np.linspace(0, drop_path_rate, self.num_layers)\n",
    "\n",
    "        self.layers = ModuleList()\n",
    "        if isinstance(layer_cfgs, dict):\n",
    "            layer_cfgs = [layer_cfgs] * self.num_layers\n",
    "        ssm_cfg={\"d_state\":d_state}\n",
    "        if use_v2 and is_2d:\n",
    "            ssm_cfg['use_v2'] = use_v2\n",
    "        if force_a2:\n",
    "            ssm_cfg['force_a2'] = force_a2\n",
    "        if dt_scale > 0:\n",
    "            ssm_cfg['dt_scale'] = dt_scale\n",
    "        if dt_scale_tmp > 0 and (i//2)%3==2:\n",
    "            ssm_cfg['dt_scale'] = dt_scale\n",
    "        for i in range(self.num_layers):\n",
    "            _layer_cfg = dict(\n",
    "                embed_dims=self.embed_dims,\n",
    "                num_heads=self.arch_settings['num_heads'],\n",
    "                feedforward_channels=self.\n",
    "                arch_settings['feedforward_channels'],\n",
    "                layer_scale_init_value=layer_scale_init_value,\n",
    "                drop_rate=drop_rate,\n",
    "                drop_path_rate=dpr[i],\n",
    "                qkv_bias=qkv_bias,\n",
    "                norm_cfg=norm_cfg)\n",
    "            _layer_cfg.update(layer_cfgs[i])\n",
    "            #self.layers.append(TransformerEncoderLayer(**_layer_cfg))\n",
    "            self.layers.append(\n",
    "                create_block(\n",
    "                d_model=self.embed_dims,\n",
    "                ssm_cfg=ssm_cfg,\n",
    "                fused_add_norm=fused_add_norm,\n",
    "                residual_in_fp32=True,\n",
    "                drop_rate=drop_rate,\n",
    "                drop_path_rate=dpr[i],\n",
    "                reverse= (not split_head ) and (i % 2) > 0,\n",
    "                transpose = (not split_head ) and has_transpose and ( i % 4) >=2,\n",
    "                use_mlp=use_mlp,\n",
    "                is_2d=is_2d,\n",
    "                rms_norm=False,\n",
    "                split_head=split_head,\n",
    "                use_nd=self.use_nd\n",
    "                )\n",
    "            )\n",
    "        self.frozen_stages = frozen_stages\n",
    "        if pre_norm:\n",
    "            self.pre_norm = build_norm_layer(norm_cfg, self.embed_dims)\n",
    "        else:\n",
    "            self.pre_norm = nn.Identity()\n",
    "\n",
    "        self.final_norm = final_norm\n",
    "        if self.out_type == 'avg_featmap':\n",
    "            self.ln1 = nn.Identity()\n",
    "            self.ln2 = build_norm_layer(norm_cfg_2, self.embed_dims)\n",
    "        elif final_norm:\n",
    "            self.ln1 = build_norm_layer(norm_cfg, self.embed_dims)\n",
    "        # if self.out_type == 'avg_featmap':\n",
    "        #     self.ln2 = build_norm_layer(norm_cfg_2, self.embed_dims)\n",
    "\n",
    "        # freeze stages only when self.frozen_stages > 0\n",
    "        if self.frozen_stages > 0:\n",
    "            self._freeze_stages()\n",
    "        self.count_parameters()\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def norm1(self):\n",
    "        return self.ln1\n",
    "\n",
    "    @property\n",
    "    def norm2(self):\n",
    "        return self.ln2\n",
    "\n",
    "    def init_weights(self):\n",
    "\n",
    "        if not (isinstance(self.init_cfg, dict)\n",
    "                and self.init_cfg['type'] == 'Pretrained'):\n",
    "            if self.pos_embed is not None:\n",
    "                trunc_normal_(self.pos_embed, std=0.02)\n",
    "        if self.pretrained2d:\n",
    "            logger = MMLogger.get_current_instance()\n",
    "            logger.info(f'load model from: {self.pretrained}')\n",
    "            # Inflate 2D model into 3D model.\n",
    "            self.inflate_weights(logger)\n",
    "        else:\n",
    "            if self.pretrained:\n",
    "                self.init_cfg = dict(\n",
    "                    type='Pretrained', checkpoint=self.pretrained)\n",
    "            super().init_weights()\n",
    "\n",
    "    def _prepare_pos_embed(self, state_dict, prefix, *args, **kwargs):\n",
    "        name = prefix + 'pos_embed'\n",
    "        if name not in state_dict.keys():\n",
    "            return\n",
    "\n",
    "        ckpt_pos_embed_shape = state_dict[name].shape\n",
    "        if (not self.with_cls_token\n",
    "                and ckpt_pos_embed_shape[1] == self.pos_embed.shape[1] + 1):\n",
    "            # Remove cls token from state dict if it's not used.\n",
    "            state_dict[name] = state_dict[name][:, 1:]\n",
    "            ckpt_pos_embed_shape = state_dict[name].shape\n",
    "\n",
    "        if self.pos_embed.shape != ckpt_pos_embed_shape:\n",
    "            from mmengine.logging import MMLogger\n",
    "            logger = MMLogger.get_current_instance()\n",
    "            logger.info(\n",
    "                f'Resize the pos_embed shape from {ckpt_pos_embed_shape} '\n",
    "                f'to {self.pos_embed.shape}.')\n",
    "\n",
    "            ckpt_pos_embed_shape = to_2tuple(\n",
    "                int(np.sqrt(ckpt_pos_embed_shape[1] - self.num_extra_tokens)))\n",
    "            pos_embed_shape = self.patch_embed.init_out_size\n",
    "\n",
    "            state_dict[name] = resize_pos_embed(state_dict[name],\n",
    "                                                ckpt_pos_embed_shape,\n",
    "                                                pos_embed_shape,\n",
    "                                                self.interpolate_mode,\n",
    "                                                self.num_extra_tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_pos_embed(*args, **kwargs):\n",
    "        \"\"\"Interface for backward-compatibility.\"\"\"\n",
    "        return resize_pos_embed(*args, **kwargs)\n",
    "\n",
    "    def _freeze_stages(self):\n",
    "        # freeze position embedding\n",
    "        if self.pos_embed is not None:\n",
    "            self.pos_embed.requires_grad = False\n",
    "        # set dropout to eval model\n",
    "        self.drop_after_pos.eval()\n",
    "        # freeze patch embedding\n",
    "        self.patch_embed.eval()\n",
    "        for param in self.patch_embed.parameters():\n",
    "            param.requires_grad = False\n",
    "        # freeze pre-norm\n",
    "        for param in self.pre_norm.parameters():\n",
    "            param.requires_grad = False\n",
    "        # freeze cls_token\n",
    "        if self.cls_token is not None:\n",
    "            self.cls_token.requires_grad = False\n",
    "        # freeze layers\n",
    "        for i in range(1, self.frozen_stages + 1):\n",
    "            m = self.layers[i - 1]\n",
    "            m.eval()\n",
    "            for param in m.parameters():\n",
    "                param.requires_grad = False\n",
    "        # freeze the last layer norm\n",
    "        if self.frozen_stages == len(self.layers):\n",
    "            if self.final_norm:\n",
    "                self.ln1.eval()\n",
    "                for param in self.ln1.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            if self.out_type == 'avg_featmap':\n",
    "                self.ln2.eval()\n",
    "                for param in self.ln2.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        b, _, _, h, w = x.shape\n",
    "        # h //= self.patch_size\n",
    "        # w //= self.patch_size\n",
    "        \n",
    "        #print(\"entrée shape\", x.shape)\n",
    "        \n",
    "        x, patch_resolution = self.patch_embed(x)\n",
    "        \n",
    "        #print(\"patch shape\", x.shape)\n",
    "        \n",
    "        patch_resolution = (patch_resolution[0],patch_resolution[1],patch_resolution[1])\n",
    "\n",
    "        x = x + resize_pos_embed(\n",
    "            self.pos_embed,\n",
    "            self.patch_resolution,\n",
    "            patch_resolution,\n",
    "            mode=self.interpolate_mode,\n",
    "            num_extra_tokens=self.num_extra_tokens)[:,self.num_extra_tokens:]\n",
    "        if self.is_2d:\n",
    "            assert self.cls_token is  None\n",
    "            x = rearrange(x,'n (h w) c-> n c h w',h=patch_resolution[0],w=patch_resolution[1])\n",
    "        if self.cls_token is not None:\n",
    "            # stole cls_tokens impl from Phil Wang, thanks\n",
    "            cls_token = self.cls_token.expand(B, -1, -1)\n",
    "            x = torch.cat((x,cls_token), dim=1) # append last\n",
    "        x = self.drop_after_pos(x)\n",
    "\n",
    "        x = self.pre_norm(x)\n",
    "\n",
    "        outs = []\n",
    "        residual = None\n",
    "        orders = (\n",
    "                't l h w',\n",
    "                't l w h',\n",
    "                'w h t l'\n",
    "        )\n",
    "        if self.is_1d:\n",
    "            orders = (\n",
    "                't l h w',\n",
    "                't l h w',\n",
    "                't l h w',\n",
    "            )\n",
    "        if self.force_2d:\n",
    "            orders = (\n",
    "                't l h w',\n",
    "                't l w h',\n",
    "                't l h w',\n",
    "            )\n",
    "            \n",
    "        n_dim_pos = [self.n_dim_pos ] * 3\n",
    "\n",
    "        if self.factorization is not None:\n",
    "            if self.factorization == 'hw_t':\n",
    "                n_dim_pos = (2,2,4)\n",
    "            elif self.factorization == 'h_w_t':\n",
    "                n_dim_pos = (1,1,2)\n",
    "        shape = (patch_resolution[0],1,patch_resolution[1],patch_resolution[2])\n",
    "        raw_x = 0\n",
    "        if self.update_interval:\n",
    "            raw_x = x\n",
    "            for i,blk in enumerate(self.layers):\n",
    "                z = i // 2\n",
    "                d = z % len(orders)\n",
    "                x = x + blk(raw_x,order=orders[d],shape=shape,skip=False,n_dim_pos=n_dim_pos[d])\n",
    "                if (i+1) % self.update_interval == 0 or i == len(self.layers) - 1:\n",
    "                    raw_x = x\n",
    "                #x = raw_x\n",
    "                if i == len(self.layers) - 1:\n",
    "                    x = (x + residual) if residual is not None else x\n",
    "                if i == len(self.layers) - 1 and self.final_norm:\n",
    "                    x = self.ln1(x)\n",
    "\n",
    "                if i in self.out_indices:\n",
    "                    outs.append(self._format_output(x, patch_resolution))\n",
    "        else:\n",
    "            \n",
    "            # print(\"x shape avant layer\", x.shape)\n",
    "            \n",
    "            for i,blk in enumerate(self.layers):\n",
    "                z = i // 2\n",
    "                d = z % len(orders)\n",
    "\n",
    "                # print(i)\n",
    "                # print(\"orders\", orders[d])\n",
    "                x = blk(x,order=orders[d],shape=shape,n_dim_pos=n_dim_pos[d])\n",
    "                    \n",
    "            # for i, layer in enumerate(self.layers):\n",
    "            #     x,residual = layer(x,residual)\n",
    "\n",
    "                if i == len(self.layers) - 1:\n",
    "                    x = (x + residual) if residual is not None else x\n",
    "                if i == len(self.layers) - 1 and self.final_norm:\n",
    "                    x = self.ln1(x)\n",
    "\n",
    "                if i in self.out_indices:\n",
    "                    outs.append(self._format_output(x, patch_resolution))\n",
    "        return outs[-1]\n",
    "    \n",
    "    def count_parameters(self,model=None):\n",
    "        if model is None:\n",
    "            model = self\n",
    "        table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "        total_params = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if not parameter.requires_grad:\n",
    "                continue\n",
    "            params = parameter.numel()\n",
    "            table.add_row([name, params])\n",
    "            total_params += params\n",
    "        self.total_parms = total_params\n",
    "        print(table)\n",
    "        print(f\"Total Trainable Params: {total_params}\")\n",
    "        return total_params\n",
    "\n",
    "    def inflate_weights(self, logger: MMLogger) -> None:\n",
    "        \"\"\"Inflate the swin2d parameters to swin3d.\n",
    "\n",
    "        The differences between swin3d and swin2d mainly lie in an extra\n",
    "        axis. To utilize the pretrained parameters in 2d model, the weight\n",
    "        of swin2d models should be inflated to fit in the shapes of the\n",
    "        3d counterpart.\n",
    "\n",
    "        Args:\n",
    "            logger (MMLogger): The logger used to print debugging information.\n",
    "        \"\"\"\n",
    "        if not self.pretrained:\n",
    "            return\n",
    "        checkpoint = _load_checkpoint(self.pretrained, map_location='cpu')\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        state_dict = {k.replace('backbone.',''):v for k,v in state_dict.items()}\n",
    "        curr_state_dict = self.state_dict()\n",
    "        if self.inflate_len:\n",
    "            new_weights = {}\n",
    "            for k,v in state_dict.items():\n",
    "                if 'layer' in k:\n",
    "                    i_layer = int(re.compile('layers.([0-9]+).').findall(k)[0])\n",
    "                    # 0 1 2 3 x x 4 5 6 7 x x\n",
    "                    n_blk = i_layer // 4\n",
    "                    n_idx = i_layer % 4\n",
    "                    new_idx = n_blk * 6 + n_idx\n",
    "                    k1 = k.replace(f'layers.{i_layer}',f'layers.{new_idx}')\n",
    "                    assert k1 not in new_weights\n",
    "                    new_weights[k1] = v\n",
    "                    if self.copy_weight:\n",
    "                        if  n_idx in [2,3]:\n",
    "                            k2 = k.replace(f'layers.{i_layer}',f'layers.{new_idx+2}')\n",
    "                            new_weights[k2] = v\n",
    "                else:\n",
    "                    new_weights[k] = v\n",
    "            state_dict = new_weights\n",
    "        for k in curr_state_dict:\n",
    "            if k in state_dict:\n",
    "                if (shape_1:=curr_state_dict[k].shape) != (shape_2:=state_dict[k].shape):\n",
    "                    if 'patch_embed' in k:\n",
    "                        state_dict[k] = state_dict[k].unsqueeze(-3).repeat(1,1,shape_1[2],1,1) / shape_1[2]\n",
    "                        assert state_dict[k].shape ==shape_1\n",
    "                    elif 'pos_embed' in k:\n",
    "                        old_len = state_dict[k].shape[1]\n",
    "                        state_dict[k] = state_dict[k].repeat(1,self.patch_resolution[0],1) #/ self.patch_resolution[0]\n",
    "                        idxes = torch.arange(self.patch_resolution[0]).view(1,-1,1).repeat(1,1,old_len).view(1,-1,1).float()\n",
    "                        if self.inlfate_policy == 'cosine':\n",
    "                            state_dict[k]  = state_dict[k]  * torch.cos(idxes / self.patch_resolution[0] * np.pi)\n",
    "                        elif self.inlfate_policy == 'single':\n",
    "                            state_dict[k]  = state_dict[k]  * (idxes ==( self.patch_resolution[0]//2))\n",
    "                        assert state_dict[k].shape ==shape_1,(state_dict[k].shape,shape_1)\n",
    "                    else:\n",
    "                        print(k,shape_1,shape_2)\n",
    "            else:\n",
    "                print(k)\n",
    "                #re.compile('')\n",
    "        # delete relative_position_index since we always re-init it\n",
    "        \n",
    "        # bicubic interpolate relative_position_bias_table if not match\n",
    "        # relative_position_bias_table_keys = [\n",
    "        #     k for k in state_dict.keys() if 'relative_position_bias_table' in k\n",
    "        # ]\n",
    "        # for k in relative_position_bias_table_keys:\n",
    "        #     relative_position_bias_table_pretrained = state_dict[k]\n",
    "        #     relative_position_bias_table_current = self.state_dict()[k]\n",
    "        #     L1, nH1 = relative_position_bias_table_pretrained.size()\n",
    "        #     L2, nH2 = relative_position_bias_table_current.size()\n",
    "        #     L2 = (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
    "        #     wd = self.window_size[0]\n",
    "        #     if nH1 != nH2:\n",
    "        #         logger.warning(f'Error in loading {k}, passing')\n",
    "        #     else:\n",
    "        #         if L1 != L2:\n",
    "        #             S1 = int(L1**0.5)\n",
    "        #             relative_position_bias_table_pretrained_resized = \\\n",
    "        #                 torch.nn.functional.interpolate(\n",
    "        #                     relative_position_bias_table_pretrained.permute(\n",
    "        #                         1, 0).view(1, nH1, S1, S1),\n",
    "        #                     size=(2 * self.window_size[1] - 1,\n",
    "        #                           2 * self.window_size[2] - 1),\n",
    "        #                     mode='bicubic')\n",
    "        #             relative_position_bias_table_pretrained = \\\n",
    "        #                 relative_position_bias_table_pretrained_resized. \\\n",
    "        #                 view(nH2, L2).permute(1, 0)\n",
    "        #     state_dict[k] = relative_position_bias_table_pretrained.repeat(\n",
    "        #         2 * wd - 1, 1)\n",
    "\n",
    "        # In the original swin2d checkpoint, the last layer of the\n",
    "        # backbone is the norm layer, and the original attribute\n",
    "        # name is `norm`. We changed it to `norm3` which means it\n",
    "        # is the last norm layer of stage 4.\n",
    "        # if hasattr(self, 'norm3'):\n",
    "        #     state_dict['norm3.weight'] = state_dict['norm.weight']\n",
    "        #     state_dict['norm3.bias'] = state_dict['norm.bias']\n",
    "        #     del state_dict['norm.weight']\n",
    "        #     del state_dict['norm.bias']\n",
    "\n",
    "        msg = self.load_state_dict(state_dict, strict=False)\n",
    "        logger.info(msg)\n",
    "\n",
    "    def _format_output(self, x, hw):\n",
    "        if self.out_type == 'raw':\n",
    "            return x\n",
    "        if self.out_type == 'cls_token':\n",
    "            return x[:, -1]\n",
    "        if not self.is_2d:\n",
    "            patch_token = x[:, self.num_extra_tokens:]\n",
    "        else:\n",
    "            patch_token = x\n",
    "        if self.out_type == 'featmap':\n",
    "            B = x.size(0)\n",
    "            # (B, N, C) -> (B, H, W, C) -> (B, C, H, W)\n",
    "            if self.is_2d:\n",
    "                return patch_token\n",
    "            else:\n",
    "                return patch_token.reshape(B, *hw, -1).permute(0, 4, 1, 2,3)\n",
    "        if self.out_type == 'avg_featmap':\n",
    "            if self.is_2d:\n",
    "                return self.ln2(patch_token.mean(dim=1))\n",
    "            else:\n",
    "                return self.ln2(patch_token.mean(dim=1))\n",
    "\n",
    "    def get_layer_depth(self, param_name: str, prefix: str = ''):\n",
    "        \"\"\"Get the layer-wise depth of a parameter.\n",
    "\n",
    "        Args:\n",
    "            param_name (str): The name of the parameter.\n",
    "            prefix (str): The prefix for the parameter.\n",
    "                Defaults to an empty string.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: The layer-wise depth and the num of layers.\n",
    "\n",
    "        Note:\n",
    "            The first depth is the stem module (``layer_depth=0``), and the\n",
    "            last depth is the subsequent module (``layer_depth=num_layers-1``)\n",
    "        \"\"\"\n",
    "        num_layers = self.num_layers + 2\n",
    "\n",
    "        if not param_name.startswith(prefix):\n",
    "            # For subsequent module like head\n",
    "            return num_layers - 1, num_layers\n",
    "\n",
    "        param_name = param_name[len(prefix):]\n",
    "\n",
    "        if param_name in ('cls_token', 'pos_embed'):\n",
    "            layer_depth = 0\n",
    "        elif param_name.startswith('patch_embed'):\n",
    "            layer_depth = 0\n",
    "        elif param_name.startswith('layers'):\n",
    "            layer_id = int(param_name.split('.')[1])\n",
    "            layer_depth = layer_id + 1\n",
    "        else:\n",
    "            layer_depth = num_layers - 1\n",
    "\n",
    "        return layer_depth, num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc129cc6-de0e-4e43-891b-9fcc530ae5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tonic\n",
    "# pip install dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a541705b-bb15-4e79-b7fd-fb0e9413fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tonic\n",
    "print(tonic.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e924e76-49f1-4f51-b6ad-1ba3dc97b323",
   "metadata": {},
   "source": [
    "### DATALOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331e10e-6390-49b8-86bc-26e288ecb40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy.lib import recfunctions\n",
    "import dv\n",
    "import torch\n",
    "from typing import Tuple, Any\n",
    "from tonic.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import struct\n",
    "import tonic\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "class DataSample:\n",
    "    def __init__(self, gt_label):\n",
    "        self.gt_label = torch.tensor([gt_label])  # Assurez-vous que gt_label est un tenseur 1D\n",
    "        self.pred_score = None\n",
    "        self.pred_label = None\n",
    "\n",
    "    def set_pred_score(self, score):\n",
    "       self.pred_score = score\n",
    "       return self\n",
    "\n",
    "    def set_pred_label(self, label):\n",
    "        self.pred_label = label\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.lib import recfunctions\n",
    "import scipy.io as scio\n",
    "from typing import Tuple, Any, Optional\n",
    "from tonic.dataset import Dataset\n",
    "from tonic.download_utils import extract_archive\n",
    "\n",
    "\n",
    "class UCF101DVS(Dataset):\n",
    "    \"\"\"ASL-DVS dataset <https://github.com/PIX2NVS/NVS2Graph>. Events have (txyp) ordering.\n",
    "    ::\n",
    "\n",
    "        @inproceedings{bi2019graph,\n",
    "            title={Graph-based Object Classification for Neuromorphic Vision Sensing},\n",
    "            author={Bi, Y and Chadha, A and Abbas, A and and Bourtsoulatze, E and Andreopoulos, Y},\n",
    "            booktitle={2019 IEEE International Conference on Computer Vision (ICCV)},\n",
    "            year={2019},\n",
    "            organization={IEEE}\n",
    "        }\n",
    "\n",
    "    Parameters:\n",
    "        save_to (string): Location to save files to on disk.\n",
    "        transform (callable, optional): A callable of transforms to apply to the data.\n",
    "        target_transform (callable, optional): A callable of transforms to apply to the targets/labels.\n",
    "    \"\"\"\n",
    "\n",
    "    sensor_size = (240, 180, 2)\n",
    "    dtype = np.dtype([(\"t\", int), (\"x\", int), (\"y\", int), (\"p\", int)])\n",
    "    ordering = dtype.names\n",
    "    folder_name = 'UCF101DVS'\n",
    "    def __init__(self, save_to, train=False, transform=None, target_transform=None):\n",
    "        super(UCF101DVS, self).__init__(\n",
    "            save_to, transform=transform, target_transform=target_transform\n",
    "        )\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise NotImplementedError(\n",
    "                'Please manually download the dataset from'\n",
    "                ' https://www.dropbox.com/sh/ie75dn246cacf6n/AACoU-_zkGOAwj51lSCM0JhGa?dl=0 '\n",
    "                'and extract it to {}'.format(self.location_on_system))\n",
    "\n",
    "        classes = os.listdir(self.location_on_system)\n",
    "        self.int_classes = dict(zip(classes, range(len(classes))))\n",
    "\n",
    "        for path, dirs, files in os.walk(self.location_on_system):\n",
    "            dirs.sort()\n",
    "            files.sort()\n",
    "            for file in files:\n",
    "                if file.endswith(\"mat\"):\n",
    "                    fsize = os.path.getsize(path + '/' + file) / float(1024)\n",
    "                    if fsize < 1:\n",
    "                        # print('{} size {} K'.format(file, fsize))\n",
    "                        continue\n",
    "                    self.data.append(path + \"/\" + file)\n",
    "                    self.targets.append(self.int_classes[path.split('/')[-1]])\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        events = scio.loadmat(self.data[index])\n",
    "        target = self.targets[index]\n",
    "        # print(f\"Loaded events type: {type(events)}\")\n",
    "        # print(f\"Events keys: {events.keys() if isinstance(events, dict) else 'N/A'}\")\n",
    "    \n",
    "        events = np.column_stack(\n",
    "            [\n",
    "                events[\"ts\"],\n",
    "                events[\"x\"],\n",
    "                self.sensor_size[1] - 1 - events[\"y\"],\n",
    "                events[\"pol\"],\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "        events = np.lib.recfunctions.unstructured_to_structured(events, self.dtype)\n",
    "        \n",
    "        # Check if events is of expected dtype\n",
    "        # print(f\"Events dtype after structuring: {events.dtype}\")\n",
    "       \n",
    "        # if events.dtype.type is np.void:       \n",
    "        #     logging.warning(\"Les données sont de type numpy.void, ce qui pourrait causer des problèmes.\")\n",
    "    \n",
    "        try:\n",
    "            event_data = prepare_data(events)\n",
    "            \n",
    "        except TypeError as e:\n",
    "            logging.warning(f\"Error processing file : {e} dans {self.data[index]}\")\n",
    "            # Return None or empty tensor if the transformation fails\n",
    "            event_data = None\n",
    "        \n",
    "        return event_data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _check_exists(self):\n",
    "        print(self.folder_name)\n",
    "        return self._folder_contains_at_least_n_files_of_type(\n",
    "            13523, \".mat\"\n",
    "        )\n",
    "\n",
    "\n",
    "# # Liste des fichiers corrompus\n",
    "# corrupted_files = [\n",
    "#     \"\"\n",
    "#     # to solve erreur : unpack requires a buffer of 4 bytes\n",
    "#     ]\n",
    "\n",
    "# Path to the HMDB-DVS dataset\n",
    "save_to = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy.lib import recfunctions as rfn\n",
    "import torchvision.transforms as transforms\n",
    "import tonic.transforms\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(events):\n",
    "    sensor_size = (240, 180, 2)\n",
    "    num_windows = 32\n",
    "    n_time_bins = 3\n",
    "    \n",
    "    # Trouver le premier timestamp non nul\n",
    "    #first_non_zero_t = events['t'][events['t'] > 0].min()\n",
    "    \n",
    "    # Définir la fenêtre de temps pour chaque segment\n",
    "    time_window = 100000  # Ajustez cette valeur selon vos besoins\n",
    "    \n",
    "\n",
    "    # Découper les événements en 32 parties de 100ms\n",
    "    voxel_grid_list = []\n",
    "    min_time, max_time = events['t'].min() + 100000, events['t'].max()\n",
    "    time_slices = np.linspace(min_time, min_time + (num_windows * time_window), num=33)  # 32 tranches\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        start_time = time_slices[i]\n",
    "        end_time = time_slices[i + 1]\n",
    "\n",
    "        # Sélectionner les événements dans l'intervalle de temps courant\n",
    "        mask = (events['t'] >= start_time) & (events['t'] < end_time)\n",
    "        events_slice = events[mask]\n",
    "\n",
    "\n",
    "        # Convertir cette tranche d'événements en grille de voxels (3 bins temporels)\n",
    "        transform = tonic.transforms.Compose([\n",
    "             tonic.transforms.DropEvent(p=0.2),\n",
    "             #tonic.transforms.DropEventByArea(sensor_size=sensor_size, area_ratio=0.1),\n",
    "             #tonic.transforms.RandomFlipUD(sensor_size=sensor_size, p=1),\n",
    "             tonic.transforms.RandomFlipLR(sensor_size=sensor_size, p=0.5),\n",
    "             tonic.transforms.ToVoxelGrid(sensor_size=sensor_size, n_time_bins=n_time_bins),\n",
    "             tonic.transforms.NumpyAsType(np.float32),  # Assurez-vous que les données sont converties en float32\n",
    "        ])\n",
    "        \n",
    "   \n",
    "        try:\n",
    "            voxel_grid = transform(events_slice)\n",
    "        except IndexError as e:\n",
    "            #logging.error(f\"Erreur lors de la transformation des événements dans la tranche {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        if voxel_grid.shape == (0,):\n",
    "            #logging.warning(f\"La transformation a échoué pour la tranche {i} (voxel_grid vide).\")\n",
    "            return None\n",
    "            \n",
    "        voxel_grid = torch.tensor(voxel_grid, dtype=torch.float32)\n",
    "        #logging.info(f\"Shape of voxel_grid tensor: {voxel_grid.shape}\")\n",
    "\n",
    "        # Redimensionner les images\n",
    "        resize_transform = transforms.Resize((224, 224), antialias=True)\n",
    "        voxel_grid = resize_transform(voxel_grid)\n",
    "        #logging.info(f\"Shape of voxel_grid resize: {voxel_grid.shape}\")\n",
    "\n",
    "        # Supprimer la dimension inutile\n",
    "        voxel_grid = voxel_grid.squeeze(1)  # Suppression de la dimension 1 inutile\n",
    "             \n",
    "        voxel_grid_list.append(voxel_grid)\n",
    "\n",
    "\n",
    "    # Agréger toutes les grilles de voxels pour obtenir un tensor final [1, 3, 32, 224, 224]\n",
    "    final_tensor = torch.stack(voxel_grid_list, dim=1)  # Stack along the second dimension\n",
    "    \n",
    "    # Rajouter la dimension de batch\n",
    "    final_tensor = final_tensor.unsqueeze(0)\n",
    "    \n",
    "\n",
    "    return final_tensor\n",
    "\n",
    "            \n",
    "# Initialisation du dataset\n",
    "dataset = UCF101DVS(save_to)\n",
    "\n",
    "\n",
    "\n",
    "# Séparation en ensembles d'entraînement et de validation\n",
    "train_indices, val_indices = train_test_split(\n",
    "    np.arange(len(dataset)),\n",
    "    train_size=9537,\n",
    "    test_size=3783,\n",
    "    stratify=dataset.targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "\n",
    "#to solve error : \"Expected input type to be list of tensor\"\n",
    "def custom_collate_fn(batch):\n",
    "    inputs = [item[0] for item in batch if item[0] is not None]\n",
    "    data_samples = [DataSample(item[1]) for item in batch if item[0] is not None]   \n",
    "    return {'inputs': inputs, 'data_samples': data_samples}\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_samples(dataloader, num_samples=5, mode='train'):\n",
    "    \"\"\"Affiche des échantillons d'un dataloader avec leurs labels.\"\"\"\n",
    "    # Obtenez un itérateur du dataloader\n",
    "    iterator = iter(dataloader)\n",
    "    \n",
    "    # Itérer sur les premiers num_samples éléments du dataloader\n",
    "    for i in range(num_samples):\n",
    "        sample = next(iterator)\n",
    "        inputs = sample['inputs']  # Liste de tenseurs\n",
    "        data_samples = sample['data_samples']  # Liste de DataSample\n",
    "\n",
    "        input_tensor = inputs[0]  # On prend le premier échantillon du batch\n",
    "        label = data_samples[0].gt_label.item()  # On prend la première étiquette du batch\n",
    "\n",
    "        # Affichez la forme des données pour débogage\n",
    "        print(f\"Shape of inputs: {input_tensor.shape}\")\n",
    "        # Shape of inputs: torch.Size([3, 32, 224, 224])\n",
    "        \n",
    "        # Sélectionner la première tranche temporelle et le premier canal\n",
    "        img = input_tensor[0, 2, 5, :, :]  # (C, T, H, W) -> (H, W)\n",
    "        \n",
    "        # Convertir le tensor en numpy array\n",
    "        img = img.numpy()\n",
    "        \n",
    "        # Affichez l'image\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.imshow(img, cmap='viridis')\n",
    "        plt.title(f'{mode.capitalize()} Sample {i+1} - Label: {label}')\n",
    "        plt.axis('off')\n",
    "        plt.show()     \n",
    "\n",
    "\n",
    "# Créer les dataloaders pour l'entraînement et la validation #\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=8, pin_memory=True, collate_fn=custom_collate_fn) # num_workers=4, pin_memory=True, collate_fn=custom_collate_fn\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=8, pin_memory=True, collate_fn=custom_collate_fn) # num_workers=0, persistent_workers=False\n",
    "\n",
    "# Affichage de quelques informations\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Training samples: {len(train_dataset)}, = {train_indices}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}, = {val_indices}\")\n",
    "\n",
    "# Affichez des échantillons du jeu d'entraînement\n",
    "print(\"Training Samples:\")\n",
    "show_samples(train_dataloader, num_samples=1, mode='train')\n",
    "\n",
    "# Affichez des échantillons du jeu de validation\n",
    "print(\"Validation Samples:\")\n",
    "show_samples(val_dataloader, num_samples=1, mode='val')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440ada5-460b-4ed4-bd2b-ee451177875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mmengine.hooks import Hook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "from mmengine.evaluator import BaseMetric\n",
    "from mmaction.registry import METRICS\n",
    "\n",
    "def val_step(self, data_batch):\n",
    "    # Forward pass\n",
    "    outputs = self(**data_batch)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()  # Change this to your loss function\n",
    "    loss = loss_fn(outputs, data_batch['labels'])\n",
    "\n",
    "    # Collect other metrics if needed\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'preds': outputs\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Ajout d'une metric pour le calcul de la loss de validation\n",
    "@METRICS.register_module()\n",
    "class LossValMetric(BaseMetric):\n",
    "    \"\"\"Loss evaluation metric.\"\"\"\n",
    "    default_prefix: Optional[str] = 'loss'\n",
    "\n",
    "    def __init__(self, collect_device: str = 'cpu', prefix: Optional[str] = None) -> None:\n",
    "        super().__init__(collect_device=collect_device, prefix=prefix)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()  # loss function \n",
    "\n",
    "    def process(self, data_batch: Sequence[Tuple[Any, Dict]], data_samples: Sequence[Dict]) -> None:\n",
    "        \"\"\"Process one batch of data samples and data_samples. The processed\n",
    "        results should be stored in ``self.results``, which will be used to\n",
    "        compute the metrics when all batches have been processed.\n",
    "        \"\"\"\n",
    "        outputs = [data_sample['pred_score'].to('cuda') for data_sample in data_samples]\n",
    "        labels = [data_sample['gt_label'].to('cuda') for data_sample in data_samples]\n",
    "   \n",
    "        # Stack outputs and labels to form tensors\n",
    "        outputs = torch.stack(outputs)\n",
    "        labels = torch.cat(labels)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn(outputs, labels).cpu().item()\n",
    "        self.results.append({'loss': loss})\n",
    "\n",
    "    def compute_metrics(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"Compute the metrics from processed results.\"\"\"\n",
    "        losses = [x['loss'] for x in results]\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        return {'loss': avg_loss}\n",
    "\n",
    "\n",
    "\n",
    "class MetricLoggerHook(Hook):\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_top1_acc = []\n",
    "        self.val_top1_acc = []\n",
    "        self.batch_losses = []  # To store batch losses for averaging\n",
    "        self.batch_top1_acc = []  # To store batch accuracies for averaging\n",
    "        self.all_labels = []  # To store true labels for confusion matrix\n",
    "        self.all_preds = []  # To store predicted labels for confusion matrix\n",
    "        # self.class_names = [\"ApplyEyeMakeup\",\"ApplyLipstick\",\"Archery\",\"BabyCrawling\",\"BalanceBeam\",\"BandMarching\",\"BaseballPitch\",\"Basketball\",\n",
    "        #     \"BasketballDunk\",\"BenchPress\",\"Biking\",\"Billiards\",\"BlowDryHair\",\"BlowingCandles\",\"BodyWeightSquats\",\"Bowling\",\"BoxingPunchingBag\",\"BoxingSpeedBag\",\n",
    "        #     \"BreastStroke\",\"BrushingTeeth\",\"CleanAndJerk\",\"CliffDiving\",\"CricketBowling\",\"CricketShot\",\"CuttingInKitchen\",\"Diving\",\"Drumming\",\"Fencing\",\n",
    "        #     \"FieldHockeyPenalty\",\"FloorGymnastics\",\"FrisbeeCatch\",\"FrontCrawl\",\"GolfSwing\",\"Haircut\",\"Hammering\",\"HammerThrow\",\"HandstandPushups\",\"HandstandWalking\",\n",
    "        #     \"HeadMassage\",\"HighJump\",\"HorseRace\",\"HorseRiding\",\"HulaHoop\",\"IceDancing\",\"JavelinThrow\",\"JugglingBalls\",\"JumpingJack\",\"JumpRope\",\"Kayaking\",\"Knitting\",\n",
    "        #     \"LongJump\",\"Lunges\",\"MilitaryParade\",\"Mixing\",\"MoppingFloor\",\"Nunchucks\",\"ParallelBars\",\"PizzaTossing\",\"PlayingCello\",\"PlayingDaf\",\"PlayingDhol\",\"PlayingFlute\",\n",
    "        #     \"PlayingGuitar\",\"PlayingPiano\",\"PlayingSitar\",\"PlayingTabla\",\"PlayingViolin\",\"PoleVault\",\"PommelHorse\",\"PullUps\",\"Punch\",\"PushUps\",\"Rafting\",\"RockClimbingIndoor\",\n",
    "        #     \"RopeClimbing\",\"Rowing\",\"SalsaSpin\",\"ShavingBeard\",\"Shotput\",\"SkateBoarding\",\"Skiing\",\"Skijet\",\"SkyDiving\",\"SoccerJuggling\",\"SoccerPenalty\",\"StillRings\",\n",
    "        #     \"SumoWrestling\",\"Surfing\",\"Swing\",\"TableTennisShot\",\"TaiChi\",\"TennisSwing\",\"ThrowDiscus\",\"TrampolineJumping\",\"Typing\",\"UnevenBars\",\"VolleyballSpiking\"\n",
    "        #     \"WalkingWithDog\",\"WallPushups\",\"WritingOnBoard\",\"YoYo\", \n",
    "        #     ]\n",
    "\n",
    "    def after_train_epoch(self, runner):\n",
    "        if self.batch_losses:\n",
    "            avg_loss = sum(self.batch_losses) / len(self.batch_losses)\n",
    "            self.train_loss.append(avg_loss)\n",
    "            self.batch_losses = []  # Reset batch losses for the next epoch\n",
    "        else:\n",
    "            self.train_loss.append(None)\n",
    "\n",
    "\n",
    "        if self.batch_top1_acc:\n",
    "            avg_acc = sum(self.batch_top1_acc) / len(self.batch_top1_acc)\n",
    "            self.train_top1_acc.append(avg_acc)\n",
    "            self.batch_top1_acc = []  # Reset batch accuracies for the next epoch\n",
    "        else:\n",
    "            self.train_top1_acc.append(None)\n",
    "    \n",
    "\n",
    "    def after_val_epoch(self, runner, metrics=None):\n",
    "        if metrics and 'acc/top1' in metrics:\n",
    "            self.val_top1_acc.append(metrics['acc/top1'])\n",
    "        else:\n",
    "            self.val_top1_acc.append(None)\n",
    "\n",
    "        if metrics and 'loss/loss' in metrics:\n",
    "            self.val_loss.append(metrics['loss/loss'])\n",
    "        else:\n",
    "            self.val_loss.append(None)\n",
    "\n",
    "        self.save_to_csv('metrics.csv')\n",
    "            \n",
    "        self.plot_metrics()  # Plot metrics after each validation epoch\n",
    "        # self.plot_confusion_matrix()  # Plot confusion matrix after each validation epoch\n",
    "        self.all_labels = []  # Reset labels for the next epoch\n",
    "        self.all_preds = []  # Reset predictions for the next epoch\n",
    "\n",
    "    def after_train_iter(self, runner, batch_idx, data_batch=None, outputs=None):\n",
    "        self.batch_losses.append(outputs['loss'].item())\n",
    "        runner.message_hub.update_scalar('loss', outputs['loss'].item())\n",
    "\n",
    "        self.batch_top1_acc.append(outputs['top1_acc'].item())\n",
    "        runner.message_hub.update_scalar('top1_acc', outputs['top1_acc'].item())\n",
    "\n",
    "    def after_val_iter(self, runner, batch_idx, data_batch=None, outputs=None):\n",
    "        true_labels = [sample.gt_label.item() for sample in data_batch['data_samples']]\n",
    "        pred_labels = [output.pred_label.item() for output in outputs]\n",
    "\n",
    "        self.all_labels.extend(true_labels)\n",
    "        self.all_preds.extend(pred_labels)\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.train_loss) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Courbe de perte d'entraînement\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if len(self.train_loss) > 0:\n",
    "            plt.plot(epochs, self.train_loss, label='Training loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss over Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "        # Courbe d'accuracy de validation\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if len(self.val_top1_acc) > 0:\n",
    "            plt.plot(epochs, self.val_top1_acc, label='Validation Top-1 Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Validation Accuracy over Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # def plot_confusion_matrix(self):\n",
    "    #     cm = confusion_matrix(self.all_labels, self.all_preds, normalize='true')\n",
    "    #     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.class_names)\n",
    "        \n",
    "    #     fig, ax = plt.subplots(figsize=(25, 25))  # Adjust the figure size as needed\n",
    "    #     disp.plot(ax=ax, cmap=plt.cm.Blues, xticks_rotation=90, values_format='.1f')\n",
    "        \n",
    "    #     plt.title('Confusion Matrix')\n",
    "    #     plt.xlabel('Predicted Labels')\n",
    "    #     plt.ylabel('True Labels')\n",
    "        \n",
    "    #     # Ajuster la taille de la police\n",
    "    #     plt.xticks(fontsize=20) \n",
    "    #     plt.yticks(fontsize=20)  \n",
    "        \n",
    "    #     plt.show()\n",
    "\n",
    "    def save_to_csv(self, filename):\n",
    "        epochs = range(1, len(self.train_loss) + 1)\n",
    "\n",
    "        #print(f\"epoch : {epochs}, train_loss : {self.train_loss}, val_acc : {self.val_top1_acc}\")\n",
    "        data = {\n",
    "            'Epoch': epochs,\n",
    "            'Training Loss': self.train_loss,\n",
    "            'Validation Loss': self.val_loss,\n",
    "            'Training Top-1 Accuracy': self.train_top1_acc,\n",
    "            'Validation Top-1 Accuracy': self.val_top1_acc\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d98066-8f81-45c1-832d-28869be39904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from mmengine.config import Config\n",
    "from mmengine.registry import RUNNERS\n",
    "from mmengine.runner import Runner\n",
    "import logging\n",
    "import tonic\n",
    "import random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Définir la configuration directement\n",
    "cfg_dict = {\n",
    "    # Ajoutez ici le reste de votre configuration...\n",
    "    'auto_scale_lr': dict(base_batch_size=32, enable=False),\n",
    "    'dataset_type': 'RawframeDataset',\n",
    "    'default_hooks': dict(\n",
    "        checkpoint=dict(interval=3, max_keep_ckpts=5, save_best='auto', type='CheckpointHook'),\n",
    "        logger=dict(ignore_last=False, interval=20, type='LoggerHook'),\n",
    "        param_scheduler=dict(type='ParamSchedulerHook'),\n",
    "        runtime_info=dict(type='RuntimeInfoHook'),\n",
    "        sampler_seed=dict(type='DistSamplerSeedHook'),\n",
    "        sync_buffers=dict(type='SyncBuffersHook'),\n",
    "        timer=dict(type='IterTimerHook')),\n",
    "    'default_scope': 'mmaction',\n",
    "    'env_cfg': dict(cudnn_benchmark=False,dist_cfg=dict(backend='nccl'),mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0)),\n",
    "    'file_client_args': dict(io_backend='disk'),\n",
    "    'launcher': 'pytorch',\n",
    "    'log_level': 'INFO',\n",
    "    'log_processor': dict(by_epoch=True, type='LogProcessor', window_size=20),\n",
    "    'magnitude': 9,\n",
    "    'model': dict(\n",
    "        backbone=dict(\n",
    "            type='Mamba3DModel',\n",
    "            arch='small',\n",
    "            d_state=16,\n",
    "            drop_path_rate=0.1,\n",
    "            drop_rate=0.1,\n",
    "            final_norm=False,\n",
    "            force_a2=False,\n",
    "            fused_add_norm=False,\n",
    "            img_size=224,\n",
    "            inflate_len=True,\n",
    "            is_2d=False,\n",
    "            num_frames=32,\n",
    "            out_type='featmap',\n",
    "            patch_size=16,\n",
    "            patch_size_temporal=2,\n",
    "            pretrained='/home/jovyan/mamba/mamba_2d_s_16_pretrained.pth',\n",
    "            use_mlp=False,\n",
    "            use_nd=False,\n",
    "            use_v2=False,\n",
    "            with_cls_token=False),\n",
    "        \n",
    "        cls_head=dict(\n",
    "            average_clips='prob',\n",
    "            dropout_ratio=0.4,\n",
    "            in_channels=384,\n",
    "            num_classes=101,\n",
    "            spatial_type='avg',\n",
    "            type='I3DHead'),\n",
    "        \n",
    "        data_preprocessor=dict(\n",
    "            format_shape='NCTHW',\n",
    "            # mean=[\n",
    "            #     114.75,\n",
    "            #     114.75,\n",
    "            #     114.75,\n",
    "            # ],\n",
    "            # std=[\n",
    "            #     57.375,\n",
    "            #     57.375,\n",
    "            #     57.375,\n",
    "            # ],\n",
    "            type='ActionDataPreprocessor'),\n",
    "        \n",
    "        type='Recognizer3D'),\n",
    "    \n",
    "\n",
    "    'optim_wrapper': dict(\n",
    "        constructor='SwinOptimWrapperConstructor',\n",
    "        optimizer=dict(betas=(0.9,0.999), lr=0.0006, type='AdamW', weight_decay=0.02),\n",
    "        paramwise_cfg=dict(absolute_pos_embed=dict(decay_mult=0.0),backbone=dict(lr_mult=0.1),norm=dict(decay_mult=0.0),relative_position_bias_table=dict(decay_mult=0.0)),type='AmpOptimWrapper'\n",
    "    ),\n",
    "    'param_scheduler': [\n",
    "        dict(\n",
    "            begin=0,\n",
    "            by_epoch=True,\n",
    "            convert_to_iter_based=True,\n",
    "            end=5,\n",
    "            start_factor=0.1,\n",
    "            type='LinearLR'),\n",
    "        dict(\n",
    "            T_max=70,\n",
    "            begin=5,\n",
    "            by_epoch=True,\n",
    "            end=70,\n",
    "            eta_min=1e-06,\n",
    "            type='CosineAnnealingLR'),\n",
    "    ],\n",
    "\n",
    "    \n",
    "    'rand_aug': '9,4',\n",
    "    'randomness': dict(deterministic=False, diff_rank_seed=False, seed=None),\n",
    "    'resume': False,\n",
    "    \n",
    "    'test_cfg': dict(type='TestLoop'),\n",
    "\n",
    "    'train_cfg': dict(max_epochs=70, type='EpochBasedTrainLoop', val_begin=1, val_interval=1),\n",
    "\n",
    "    'use_rand_erasing': False,\n",
    "\n",
    "    'val_cfg': dict(type='ValLoop'),\n",
    "\n",
    "    'val_evaluator': [dict(type='AccMetric'), dict(type='LossValMetric')],\n",
    "\n",
    "    'vis_backends': [dict(type='LocalVisBackend')],\n",
    "\n",
    "    'visualizer': dict(type='ActionVisualizer',vis_backends=[dict(type='LocalVisBackend')]),\n",
    "\n",
    "    'work_dir': './work_dirs/ucf101dvs__experiment',\n",
    "}\n",
    "\n",
    "cfg = Config(cfg_dict)\n",
    "\n",
    "# Affichage des configurations\n",
    "print(\"Train dataloader:\", train_dataloader)\n",
    "print(\"Validation dataloader:\",val_dataloader)\n",
    "print(\"Validation evaluator:\", cfg.val_evaluator)\n",
    "\n",
    "# Simuler les arguments d'entrée\n",
    "class Args:\n",
    "    work_dir = None  \n",
    "    resume = None  \n",
    "    amp = True  \n",
    "    no_validate = False\n",
    "    auto_scale_lr = False\n",
    "    seed = 42\n",
    "    diff_rank_seed = False\n",
    "    deterministic = False\n",
    "    cfg_options = None\n",
    "    launcher = 'none'\n",
    "    local_rank = 0\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def merge_args(cfg, args):\n",
    "    \"\"\"Merge CLI arguments to config.\"\"\"\n",
    "    if args.no_validate:\n",
    "        cfg.val_cfg = None\n",
    "        cfg.val_dataloader = None\n",
    "        cfg.val_evaluator = None\n",
    "\n",
    "    cfg.launcher = args.launcher\n",
    "\n",
    "    if args.work_dir is not None:\n",
    "        cfg.work_dir = args.work_dir\n",
    "    elif cfg.get('work_dir', None) is None:\n",
    "        cfg.work_dir = os.path.join('./work_dirs', 'ucf101dvs_experiment')\n",
    "\n",
    "    if args.amp:\n",
    "        cfg.optim_wrapper.type = 'AmpOptimWrapper'\n",
    "        cfg.optim_wrapper.setdefault('loss_scale', 'dynamic')\n",
    "\n",
    "    if args.resume == 'auto':\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = None\n",
    "    elif args.resume is not None:\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = args.resume\n",
    "\n",
    "    if args.auto_scale_lr:\n",
    "        cfg.auto_scale_lr.enable = True\n",
    "\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "# Merging args with the configuration\n",
    "cfg = merge_args(cfg, args)\n",
    "\n",
    "# Initialisation du hook personnalisé\n",
    "metric_logger_hook = MetricLoggerHook()\n",
    "\n",
    "# Initialisation du Runner\n",
    "runner = Runner(\n",
    "    model=cfg.model,\n",
    "    work_dir=cfg.work_dir,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    test_dataloader=val_dataloader,  # Assuming test and val dataloaders are the same\n",
    "    train_cfg=cfg.train_cfg,\n",
    "    val_cfg=cfg.val_cfg,\n",
    "    test_cfg=cfg.test_cfg,\n",
    "    auto_scale_lr=cfg.auto_scale_lr,\n",
    "    optim_wrapper=cfg.optim_wrapper,\n",
    "    param_scheduler=cfg.param_scheduler,\n",
    "    val_evaluator=cfg.val_evaluator,\n",
    "    test_evaluator=cfg.val_evaluator,\n",
    "    default_hooks=cfg.default_hooks,\n",
    "    custom_hooks=[metric_logger_hook],\n",
    "    resume=cfg.resume,\n",
    "    launcher=cfg.launcher,\n",
    "    env_cfg=cfg.env_cfg,\n",
    "    log_level=cfg.log_level,\n",
    "    visualizer=cfg.visualizer,\n",
    "    default_scope=cfg.default_scope,\n",
    "    randomness=cfg.randomness,\n",
    "    cfg=cfg\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Commencer l'entraînement\n",
    "logger.info(\"Starting training...\")\n",
    "\n",
    "runner.train()\n",
    "\n",
    "\n",
    "\n",
    "# Après l'entraînement, tracer les courbes\n",
    "metric_logger_hook.plot_metrics()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mamba_ND",
   "language": "python",
   "name": "mamband"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
