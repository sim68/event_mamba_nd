{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0967c2-f738-43b5-b50e-9a2bc006843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version PyTorch : 2.1.2\n",
      "Version CUDA utilisée par PyTorch : 11.8\n",
      "True\n",
      "8700\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Version PyTorch :\", torch.__version__)\n",
    "print(\"Version CUDA utilisée par PyTorch :\", torch.version.cuda)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3032608-c5e3-4792-8efe-ba6ce7ccf48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([2, 64, 16])\n",
      "output torch.Size([2, 64, 16])\n"
     ]
    }
   ],
   "source": [
    "##########################################\n",
    "############ TEST MAMBA LAYER  ###########\n",
    "##########################################\n",
    "\n",
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape\n",
    "print(\"input\", x.shape)\n",
    "print(\"output\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4e749e-657b-4224-b104-22b02fa00d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "from typing import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from mmcv.cnn.bricks.transformer import FFN, PatchEmbed\n",
    "from mmengine.model import BaseModule, ModuleList\n",
    "from mmengine.model.weight_init import trunc_normal_\n",
    "\n",
    "from mmpretrain.registry import MODELS\n",
    "from mmpretrain.models.utils import (MultiheadAttention, SwiGLUFFNFused, build_norm_layer,\n",
    "                     resize_pos_embed, to_2tuple)\n",
    "from mmpretrain.models.backbones.base_backbone import BaseBackbone\n",
    "from mamba_ssm import Mamba\n",
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
    "from functools import partial\n",
    "#from .ssm2d import Block2D,Mamba2D,SplitHead2D\n",
    "from einops import rearrange\n",
    "# from .mamband import MambaND\n",
    "from mmpretrain.models.utils.embed import PatchMerging\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "Block2D=SplitHead2D=nn.Identity # TODO: Clan implementation and release\n",
    "MambaND = nn.Identity\n",
    "\n",
    "from mmcv.cnn.bricks.drop import build_dropout\n",
    "torch.set_printoptions(precision=4)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False,reverse=False,\n",
    "        transpose=False,split_head=False,\n",
    "        drop_path_rate=0.0,drop_rate=0.0,use_mlp=False,downsample=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n",
    "\n",
    "        This Block has a slightly different structure compared to a regular\n",
    "        prenorm Transformer block.\n",
    "        The standard block is: LN -> MHA/MLP -> Add.\n",
    "        [Ref: https://arxiv.org/abs/2002.04745]\n",
    "        Here we have: Add -> LN -> Mixer, returning both\n",
    "        the hidden_states (output of the mixer) and the residual.\n",
    "        This is purely for performance reasons, as we can fuse add and LayerNorm.\n",
    "        The residual needs to be provided (except for the very first block).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        self.fused_add_norm = fused_add_norm\n",
    "        self.mixer = mixer_cls(dim)\n",
    "        self.norm = norm_cls(dim)\n",
    "        self.split_head = split_head\n",
    "        self.reverse = reverse\n",
    "        self.transpose = transpose\n",
    "        self.drop_path = build_dropout(\n",
    "            dict(type='DropPath', drop_prob=drop_path_rate)\n",
    "        )\n",
    "        self.dropout = build_dropout(\n",
    "            dict(type='Dropout', drop_prob=drop_rate)\n",
    "        )\n",
    "        self.downsample = downsample\n",
    "        if downsample:\n",
    "            self.down_sample_layer = PatchMerging(\n",
    "                dim,dim if self.split_head else dim,\n",
    "            )\n",
    "        if use_mlp:\n",
    "            self.ffn = SwiGLUFFNFused(\n",
    "                    embed_dims=dim,\n",
    "                    feedforward_channels=int(dim*4),\n",
    "                    layer_scale_init_value=0.0)\n",
    "            self.ln2 = build_norm_layer(dict(type='LN'), dim)\n",
    "        else:\n",
    "            self.ffn = None\n",
    "        if self.fused_add_norm:\n",
    "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
    "            assert isinstance(\n",
    "                self.norm, (nn.LayerNorm, RMSNorm)\n",
    "            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None,skip=True,**kwargs\n",
    "    ):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: the sequence to the encoder layer (required).\n",
    "            residual: hidden_states = Mixer(LN(residual))\n",
    "        \"\"\"\n",
    "        \n",
    "        # print(\"Input Tensor :\", hidden_states)\n",
    "        # print(\"shape : \", hidden_states.shape)\n",
    "        \n",
    "        h = w = 0\n",
    "        if self.transpose:\n",
    "            l = hidden_states.shape[1]\n",
    "            h = w = int(np.sqrt(l))\n",
    "            # assert h * w == l\n",
    "            hidden_states = rearrange(hidden_states,'n (h w) c -> n (w h) c',h=h,w=w)\n",
    "            #print(\"hidden_states transpose:\", hidden_states)\n",
    "            if residual is not None:\n",
    "                residual = rearrange(residual,'n (h w) c -> n (w h) c',h=h,w=w)\n",
    "        if self.reverse:\n",
    "            hidden_states = hidden_states.flip(1)\n",
    "            #print(\"hidden_states reverse:\", hidden_states)\n",
    "            if residual is not None:\n",
    "                residual = residual.flip(1)\n",
    "        if not self.fused_add_norm:\n",
    "            hidden_states = self.norm(hidden_states)\n",
    "            # residual = (hidden_states + residual) if residual is not None else hidden_states\n",
    "            # hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
    "            # if self.residual_in_fp32:\n",
    "            #     residual = residual.to(torch.float32)\n",
    "            if self.split_head:\n",
    "                l = hidden_states.shape[1]\n",
    "                h = w = int(np.sqrt(l))\n",
    "                hidden_states = SplitHead2D.apply(hidden_states,4,h,w)\n",
    "            if skip:\n",
    "                hidden_states = hidden_states + self.drop_path(self.mixer(hidden_states, inference_params=inference_params,**(kwargs if isinstance(self.mixer,MambaND) else {})))\n",
    "            else:\n",
    "                hidden_states = self.drop_path(self.dropout(self.mixer(hidden_states, inference_params=inference_params)))\n",
    "            if self.split_head:\n",
    "                hidden_states = SplitHead2D.apply(hidden_states,4,h,w)\n",
    "        else:\n",
    "            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n",
    "            hidden_states, residual = fused_add_norm_fn(\n",
    "                hidden_states,\n",
    "                self.norm.weight,\n",
    "                self.norm.bias,\n",
    "                residual=residual,\n",
    "                prenorm=True,\n",
    "                residual_in_fp32=self.residual_in_fp32,\n",
    "                eps=self.norm.eps,\n",
    "            )\n",
    "            hidden_states = self.drop_path(self.mixer(hidden_states, inference_params=inference_params,**kwargs))\n",
    "        if self.ffn is not None:\n",
    "            hidden_states = self.ffn(self.ln2(hidden_states),identity=hidden_states)\n",
    "        if self.reverse:\n",
    "            hidden_states = hidden_states.flip(1)\n",
    "            if residual is not None:\n",
    "                residual = residual.flip(1)\n",
    "        if self.transpose:\n",
    "            hidden_states = rearrange(hidden_states,'n (w h) c -> n (h w) c',h=h,w=w)\n",
    "            #print(\"Rearrange parcours 1\")\n",
    "            if residual is not None:\n",
    "                residual = rearrange(residual,'n (w h) c -> n (h w) c',h=h,w=w)\n",
    "        if self.downsample:\n",
    "            if 'h' in kwargs:\n",
    "                h,w = kwargs['h'],kwargs['w']\n",
    "            hidden_states,(h,w) = self.down_sample_layer(\n",
    "                hidden_states,(h,w)\n",
    "            )\n",
    "            assert residual is None\n",
    "            residual = (h,w)\n",
    "\n",
    "        # print(f\"Output Tensor : {hidden_states}\")\n",
    "        # print(\"shape : \", hidden_states.shape)\n",
    "        return hidden_states, residual\n",
    "\n",
    "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_ref\n",
    "def  causal_conv1d_fn_ref(x, weight, bias=None,seq_idx=None, activation=None):\n",
    "    \"\"\"\n",
    "    x: (batch, dim, seqlen)\n",
    "    weight: (dim, width)\n",
    "    bias: (dim,)\n",
    "    activation: either None or \"silu\" or \"swish\"\n",
    "\n",
    "    out: (batch, dim, seqlen)\n",
    "    \"\"\"\n",
    "    seqlen=x.shape[-1]\n",
    "    dim,width = weight.shape\n",
    "    assert activation =='silu'\n",
    "    x = F.conv1d(x, weight.unsqueeze(1), bias, padding=width.item() - 1, groups=dim.item())[..., :seqlen]\n",
    "    return F.silu(x)\n",
    "\n",
    "def mamba_inner_ref(\n",
    "    xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "    out_proj_weight, out_proj_bias,\n",
    "    A, B=None, C=None, D=None, delta_bias=None, B_proj_bias=None,\n",
    "    C_proj_bias=None, delta_softplus=True\n",
    "):\n",
    "    L = xz.shape[-1]\n",
    "    delta_rank = delta_proj_weight.shape[1]\n",
    "    d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "    x, z = xz.chunk(2, dim=1)\n",
    "    x = causal_conv1d_fn_ref(x, rearrange(conv1d_weight, \"d 1 w -> d w\"), bias=conv1d_bias,seq_idx=None, activation=\"silu\")\n",
    "    # We're being very careful here about the layout, to avoid extra transposes.\n",
    "    # We want delta to have d as the slowest moving dimension\n",
    "    # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "    x_dbl = F.linear(rearrange(x, 'b d l -> (b l) d'), x_proj_weight)  # (bl d)\n",
    "    delta = delta_proj_weight @ x_dbl[:, :delta_rank].t()\n",
    "    delta = rearrange(delta, \"d (b l) -> b d l\", l=L)\n",
    "    if B is None:  # variable B\n",
    "        B = x_dbl[:, delta_rank:delta_rank + d_state]  # (bl d)\n",
    "        if B_proj_bias is not None:\n",
    "            B = B + B_proj_bias.to(dtype=B.dtype)\n",
    "        if not A.is_complex():\n",
    "            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "        else:\n",
    "            B = rearrange(B, \"(b l) (dstate two) -> b dstate (l two)\", l=L, two=2).contiguous()\n",
    "    if C is None:  # variable B\n",
    "        C = x_dbl[:, -d_state:]  # (bl d)\n",
    "        if C_proj_bias is not None:\n",
    "            C = C + C_proj_bias.to(dtype=C.dtype)\n",
    "        if not A.is_complex():\n",
    "            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=L).contiguous()\n",
    "        else:\n",
    "            C = rearrange(C, \"(b l) (dstate two) -> b dstate (l two)\", l=L, two=2).contiguous()\n",
    "    y = selective_scan_ref(x, delta, A, B, C, D, z=z, delta_bias=delta_bias, delta_softplus=True)\n",
    "    return F.linear(rearrange(y, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n",
    "\n",
    "# for FLOPS calc only\n",
    "class Mamba_Ref(Mamba):\n",
    "\n",
    "    def forward(self, hidden_states, inference_params=None):\n",
    "        \"\"\"\n",
    "        hidden_states: (B, L, D)\n",
    "        Returns: same shape as hidden_states\n",
    "        \"\"\"\n",
    "        batch, seqlen, dim = hidden_states.shape\n",
    "\n",
    "        conv_state, ssm_state = None, None\n",
    "        if inference_params is not None:\n",
    "            conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)\n",
    "            if inference_params.seqlen_offset > 0:\n",
    "                # The states are updated inplace\n",
    "                out, _, _ = self.step(hidden_states, conv_state, ssm_state)\n",
    "                return out\n",
    "\n",
    "        # We do matmul and transpose BLH -> HBL at the same time\n",
    "        xz = rearrange(\n",
    "            self.in_proj.weight @ rearrange(hidden_states, \"b l d -> d (b l)\"),\n",
    "            \"d (b l) -> b d l\",\n",
    "            l=seqlen,\n",
    "        )\n",
    "        if self.in_proj.bias is not None:\n",
    "            xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), \"d -> d 1\")\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())  # (d_inner, d_state)\n",
    "        # In the backward pass we write dx and dz next to each other to avoid torch.cat\n",
    "        if self.use_fast_path and inference_params is None:  # Doesn't support outputting the states\n",
    "            out = mamba_inner_ref(\n",
    "                xz,\n",
    "                self.conv1d.weight,\n",
    "                self.conv1d.bias,\n",
    "                self.x_proj.weight,\n",
    "                self.dt_proj.weight,\n",
    "                self.out_proj.weight,\n",
    "                self.out_proj.bias,\n",
    "                A,\n",
    "                None,  # input-dependent B\n",
    "                None,  # input-dependent C\n",
    "                self.D.float(),\n",
    "                delta_bias=self.dt_proj.bias.float(),\n",
    "                delta_softplus=True,\n",
    "            )\n",
    "        else:\n",
    "            x, z = xz.chunk(2, dim=1)\n",
    "            # Compute short convolution\n",
    "            if conv_state is not None:\n",
    "                # If we just take x[:, :, -self.d_conv :], it will error if seqlen < self.d_conv\n",
    "                # Instead F.pad will pad with zeros if seqlen < self.d_conv, and truncate otherwise.\n",
    "                conv_state.copy_(F.pad(x, (self.d_conv - x.shape[-1], 0)))  # Update state (B D W)\n",
    "            if causal_conv1d_fn is None:\n",
    "                x = self.act(self.conv1d(x)[..., :seqlen])\n",
    "            else:\n",
    "                assert self.activation in [\"silu\", \"swish\"]\n",
    "                x = causal_conv1d_fn(\n",
    "                    x=x,\n",
    "                    weight=rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
    "                    bias=self.conv1d.bias,\n",
    "                    activation=self.activation,\n",
    "                )\n",
    "\n",
    "            # We're careful here about the layout, to avoid extra transposes.\n",
    "            # We want dt to have d as the slowest moving dimension\n",
    "            # and L as the fastest moving dimension, since those are what the ssm_scan kernel expects.\n",
    "            x_dbl = self.x_proj(rearrange(x, \"b d l -> (b l) d\"))  # (bl d)\n",
    "            dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=-1)\n",
    "            dt = self.dt_proj.weight @ dt.t()\n",
    "            dt = rearrange(dt, \"d (b l) -> b d l\", l=seqlen)\n",
    "            B = rearrange(B, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "            C = rearrange(C, \"(b l) dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "            assert self.activation in [\"silu\", \"swish\"]\n",
    "            y = selective_scan_ref(\n",
    "                x,\n",
    "                dt,\n",
    "                A,\n",
    "                B,\n",
    "                C,\n",
    "                self.D.float(),\n",
    "                z=z,\n",
    "                delta_bias=self.dt_proj.bias.float(),\n",
    "                delta_softplus=True,\n",
    "                return_last_state=ssm_state is not None,\n",
    "            )\n",
    "            if ssm_state is not None:\n",
    "                y, last_state = y\n",
    "                ssm_state.copy_(last_state)\n",
    "            y = rearrange(y, \"b d l -> b l d\")\n",
    "            out = self.out_proj(y)\n",
    "        return out\n",
    "\n",
    "\n",
    "def create_block(\n",
    "    d_model,\n",
    "    ssm_cfg=None,\n",
    "    norm_epsilon=1e-5,\n",
    "    rms_norm=False,\n",
    "    residual_in_fp32=False,\n",
    "    fused_add_norm=False,\n",
    "    layer_idx=None,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    "    reverse=None,\n",
    "    is_2d=False,\n",
    "    drop_rate=0.1,\n",
    "    drop_path_rate=0.1,\n",
    "    use_mlp=False,\n",
    "    transpose=False,\n",
    "    split_head=False,\n",
    "    use_nd=False,\n",
    "    downsample=False,\n",
    "    use_ref=False,\n",
    "    n_dim=2,\n",
    "):\n",
    "    if ssm_cfg is None:\n",
    "        ssm_cfg = {}\n",
    "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "    \n",
    "    if use_nd: \n",
    "        #ssm_cfg['d_state'] *= 4\n",
    "        transpose = False\n",
    "        reverse = False\n",
    "        mixer_cls = partial(MambaND , layer_idx=layer_idx, n_dim=n_dim,**ssm_cfg, **factory_kwargs)\n",
    "    elif use_ref:\n",
    "        mixer_cls = partial(Mamba_Ref, layer_idx=layer_idx, **ssm_cfg, **factory_kwargs)\n",
    "    else:\n",
    "        mixer_cls = partial(Mamba2D if is_2d else Mamba, layer_idx=layer_idx, **ssm_cfg, **factory_kwargs)\n",
    "    norm_cls = partial(\n",
    "        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs\n",
    "    )\n",
    "    if is_2d:\n",
    "        block = Block2D(\n",
    "            d_model,\n",
    "            mixer_cls,\n",
    "            norm_cls=norm_cls,\n",
    "            fused_add_norm=fused_add_norm,\n",
    "            residual_in_fp32=residual_in_fp32,\n",
    "            reverse=reverse,\n",
    "            drop_rate=drop_rate,\n",
    "            transpose=transpose,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "        )\n",
    "    else:\n",
    "        block = Block(\n",
    "            d_model,\n",
    "            mixer_cls,\n",
    "            norm_cls=norm_cls,\n",
    "            fused_add_norm=fused_add_norm,\n",
    "            residual_in_fp32=residual_in_fp32,\n",
    "            reverse=reverse,\n",
    "            transpose=transpose,\n",
    "            drop_rate=drop_rate,\n",
    "            use_mlp=use_mlp,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "            split_head=split_head,\n",
    "            downsample=downsample,\n",
    "        )\n",
    "    block.layer_idx = layer_idx\n",
    "    return block \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d5e2dcb-29eb-49e6-b2ab-f78a694ac234",
   "metadata": {},
   "outputs": [],
   "source": [
    "@MODELS.register_module()\n",
    "class Mamba2DModel(BaseBackbone):\n",
    "    \"\"\"Vision Transformer.\n",
    "\n",
    "    A PyTorch implement of : `An Image is Worth 16x16 Words: Transformers\n",
    "    for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_\n",
    "\n",
    "    Args:\n",
    "        arch (str | dict): Vision Transformer architecture. If use string,\n",
    "            choose from 'small', 'base', 'large', 'deit-tiny', 'deit-small'\n",
    "            and 'deit-base'. If use dict, it should have below keys:\n",
    "\n",
    "            - **embed_dims** (int): The dimensions of embedding.\n",
    "            - **num_layers** (int): The number of transformer encoder layers.\n",
    "            - **num_heads** (int): The number of heads in attention modules.\n",
    "            - **feedforward_channels** (int): The hidden dimensions in\n",
    "              feedforward modules.\n",
    "\n",
    "            Defaults to 'base'.\n",
    "        img_size (int | tuple): The expected input image shape. Because we\n",
    "            support dynamic input shape, just set the argument to the most\n",
    "            common input image shape. Defaults to 224.\n",
    "        patch_size (int | tuple): The patch size in patch embedding.\n",
    "            Defaults to 16.\n",
    "        in_channels (int): The num of input channels. Defaults to 3.\n",
    "        out_indices (Sequence | int): Output from which stages.\n",
    "            Defaults to -1, means the last stage.\n",
    "        drop_rate (float): Probability of an element to be zeroed.\n",
    "            Defaults to 0.\n",
    "        drop_path_rate (float): stochastic depth rate. Defaults to 0.\n",
    "        qkv_bias (bool): Whether to add bias for qkv in attention modules.\n",
    "            Defaults to True.\n",
    "        norm_cfg (dict): Config dict for normalization layer.\n",
    "            Defaults to ``dict(type='LN')``.\n",
    "        final_norm (bool): Whether to add a additional layer to normalize\n",
    "            final feature map. Defaults to True.\n",
    "        out_type (str): The type of output features. Please choose from\n",
    "\n",
    "            - ``\"cls_token\"``: The class token tensor with shape (B, C).\n",
    "            - ``\"featmap\"``: The feature map tensor from the patch tokens\n",
    "              with shape (B, C, H, W).\n",
    "            - ``\"avg_featmap\"``: The global averaged feature map tensor\n",
    "              with shape (B, C).\n",
    "            - ``\"raw\"``: The raw feature tensor includes patch tokens and\n",
    "              class tokens with shape (B, L, C).\n",
    "\n",
    "            Defaults to ``\"cls_token\"``.\n",
    "        with_cls_token (bool): Whether concatenating class token into image\n",
    "            tokens as transformer input. Defaults to True.\n",
    "        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n",
    "            -1 means not freezing any parameters. Defaults to -1.\n",
    "        interpolate_mode (str): Select the interpolate mode for position\n",
    "            embeding vector resize. Defaults to \"bicubic\".\n",
    "        layer_scale_init_value (float or torch.Tensor): Init value of layer\n",
    "            scale. Defaults to 0.\n",
    "        patch_cfg (dict): Configs of patch embeding. Defaults to an empty dict.\n",
    "        layer_cfgs (Sequence | dict): Configs of each transformer layer in\n",
    "            encoder. Defaults to an empty dict.\n",
    "        init_cfg (dict, optional): Initialization config dict.\n",
    "            Defaults to None.\n",
    "    \"\"\"\n",
    "    arch_zoo = {\n",
    "        **dict.fromkeys(\n",
    "            ['small'], {\n",
    "                'embed_dims': 384,\n",
    "                'num_layers': 8,\n",
    "                'num_heads': 6,\n",
    "                'feedforward_channels': 384 * 4\n",
    "            }),\n",
    "        **dict.fromkeys(\n",
    "            ['base'], {\n",
    "                'embed_dims': 768,\n",
    "                'num_layers': 12,\n",
    "                'num_heads': 12,\n",
    "                'feedforward_channels': 768 * 4\n",
    "            }),\n",
    "    }\n",
    "    num_extra_tokens = 1  # class token\n",
    "    OUT_TYPES = {'raw', 'cls_token', 'featmap', 'avg_featmap'}\n",
    "\n",
    "    def __init__(self,\n",
    "                 arch='small',\n",
    "                 img_size=32,\n",
    "                 patch_size=8,\n",
    "                 in_channels=3,\n",
    "                 out_indices=-1,\n",
    "                 drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 qkv_bias=True,\n",
    "                 norm_cfg=dict(type='LN', eps=1e-6),\n",
    "                 norm_cfg_2=dict(type='LN', eps=1e-6),\n",
    "                 final_norm=True,\n",
    "                 out_type='cls_token',\n",
    "                 with_cls_token=True,\n",
    "                 frozen_stages=-1,\n",
    "                 interpolate_mode='bicubic',\n",
    "                 layer_scale_init_value=0.,\n",
    "                 patch_cfg=dict(),\n",
    "                 layer_cfgs=dict(),\n",
    "                 pre_norm=False,\n",
    "                 init_cfg=None,\n",
    "                 is_2d=True,\n",
    "                 use_v2=False,\n",
    "                 force_a2=False,\n",
    "                 embed_dims=None,\n",
    "                 has_transpose=True,\n",
    "                 fused_add_norm=True,\n",
    "                 use_mlp=False,\n",
    "                 split_head=False,\n",
    "                 use_nd=False,\n",
    "                 downsample=None,\n",
    "                 expand=None,\n",
    "                 constant_dim=False,\n",
    "                 has_reverse=True,\n",
    "                 update_interval=None,\n",
    "                 duplicate=None,\n",
    "                 n_dim=2,\n",
    "                 num_layers=None,\n",
    "                 use_ref=False,\n",
    "                 d_state=16):\n",
    "        super(Mamba2DModel, self).__init__(init_cfg)\n",
    "\n",
    "        if isinstance(arch, str):\n",
    "            arch = arch.lower()\n",
    "            assert arch in set(self.arch_zoo), \\\n",
    "                f'Arch {arch} is not in default archs {set(self.arch_zoo)}'\n",
    "            self.arch_settings = self.arch_zoo[arch]\n",
    "        else:\n",
    "            essential_keys = {\n",
    "                'embed_dims', 'num_layers', 'num_heads', 'feedforward_channels'\n",
    "            }\n",
    "            assert isinstance(arch, dict) and essential_keys <= set(arch), \\\n",
    "                f'Custom arch needs a dict with keys {essential_keys}'\n",
    "            self.arch_settings = arch\n",
    "        self.update_interval = update_interval\n",
    "        self.embed_dims = self.arch_settings['embed_dims'] \n",
    "        if embed_dims is not None:\n",
    "            self.embed_dims = embed_dims\n",
    "        if num_layers is None:\n",
    "            num_layers = self.arch_settings['num_layers']\n",
    "        self.num_layers = num_layers * (2 if not use_mlp else 1)\n",
    "        self.downsample = self.arch_settings.get('downsample',downsample)\n",
    "        if self.downsample is None:\n",
    "            self.downsample = []\n",
    "        #self.downsample = list((x+1)*2-1 for x in self.downsample)\n",
    "        self.img_size = to_2tuple(img_size)\n",
    "        self.is_2d = is_2d\n",
    "        self.use_nd=use_nd\n",
    "\n",
    "        # Set patch embedding\n",
    "        _patch_cfg = dict(\n",
    "            in_channels=in_channels,\n",
    "            input_size=img_size,\n",
    "            embed_dims=self.embed_dims,\n",
    "            conv_type='Conv2d',\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "            bias=not pre_norm,  # disable bias if pre_norm is used(e.g., CLIP)\n",
    "        )\n",
    "        _patch_cfg.update(patch_cfg)\n",
    "        self.patch_embed = PatchEmbed(**_patch_cfg)\n",
    "        self.patch_resolution = self.patch_embed.init_out_size\n",
    "        num_patches = self.patch_resolution[0] * self.patch_resolution[1]\n",
    "\n",
    "        # Set out type\n",
    "        if out_type not in self.OUT_TYPES:\n",
    "            raise ValueError(f'Unsupported `out_type` {out_type}, please '\n",
    "                             f'choose from {self.OUT_TYPES}')\n",
    "        self.out_type = out_type\n",
    "\n",
    "        # Set cls token\n",
    "        self.with_cls_token = with_cls_token\n",
    "        if with_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dims))\n",
    "        elif out_type != 'cls_token':\n",
    "            self.cls_token = None\n",
    "            self.num_extra_tokens = 0\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'with_cls_token must be True when `out_type=\"cls_token\"`.')\n",
    "\n",
    "        # Set position embedding\n",
    "        self.interpolate_mode = interpolate_mode\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + self.num_extra_tokens,\n",
    "                        self.embed_dims))\n",
    "        self._register_load_state_dict_pre_hook(self._prepare_pos_embed)\n",
    "\n",
    "        self.drop_after_pos = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        if isinstance(out_indices, int):\n",
    "            out_indices = [out_indices]\n",
    "        assert isinstance(out_indices, Sequence), \\\n",
    "            f'\"out_indices\" must by a sequence or int, ' \\\n",
    "            f'get {type(out_indices)} instead.'\n",
    "        for i, index in enumerate(out_indices):\n",
    "            if index < 0:\n",
    "                out_indices[i] = self.num_layers + index\n",
    "            assert 0 <= out_indices[i] <= self.num_layers, \\\n",
    "                f'Invalid out_indices {index}'\n",
    "        self.out_indices = out_indices\n",
    "\n",
    "        # stochastic depth decay rule\n",
    "        dpr = np.linspace(0, drop_path_rate, self.num_layers)\n",
    "\n",
    "        self.layers = ModuleList()\n",
    "        if isinstance(layer_cfgs, dict):\n",
    "            layer_cfgs = [layer_cfgs] * self.num_layers\n",
    "        ssm_cfg={\"d_state\":d_state}\n",
    "        if expand is not None:\n",
    "            ssm_cfg['expand'] = expand\n",
    "        if duplicate:\n",
    "            ssm_cfg['duplicate'] = duplicate\n",
    "        if use_v2 and is_2d:\n",
    "            ssm_cfg['use_v2'] = use_v2\n",
    "        if force_a2:\n",
    "            ssm_cfg['force_a2'] = force_a2\n",
    "        dim = self.embed_dims\n",
    "        for i in range(self.num_layers):\n",
    "            #self.layers.append(TransformerEncoderLayer(**_layer_cfg))\n",
    "            do_downsample = i in self.downsample\n",
    "            self.layers.append(\n",
    "                create_block(\n",
    "                d_model=dim,\n",
    "                ssm_cfg=ssm_cfg,\n",
    "                fused_add_norm=fused_add_norm,\n",
    "                residual_in_fp32=True,\n",
    "                drop_rate=drop_rate,\n",
    "                drop_path_rate=dpr[i],\n",
    "                reverse= (not split_head ) and (i % 2) > 0 and has_reverse,\n",
    "                transpose = (not split_head ) and has_transpose and ( i % 4) >=2,\n",
    "                use_mlp=use_mlp,\n",
    "                is_2d=is_2d,\n",
    "                rms_norm=False,\n",
    "                split_head=split_head,\n",
    "                use_nd=use_nd,\n",
    "                downsample=do_downsample,\n",
    "                n_dim=n_dim,\n",
    "                use_ref=use_ref\n",
    "                )\n",
    "            )\n",
    "            if do_downsample and not constant_dim:\n",
    "                dim *= 2\n",
    "                \n",
    "        self.frozen_stages = frozen_stages\n",
    "        if pre_norm:\n",
    "            self.pre_norm = build_norm_layer(norm_cfg, dim)\n",
    "        else:\n",
    "            self.pre_norm = nn.Identity()\n",
    "\n",
    "        self.final_norm = final_norm\n",
    "        if final_norm:\n",
    "            self.ln1 = build_norm_layer(norm_cfg, dim)\n",
    "        if self.out_type == 'avg_featmap':\n",
    "            self.ln2 = build_norm_layer(norm_cfg_2, dim)\n",
    "\n",
    "        # freeze stages only when self.frozen_stages > 0\n",
    "        if self.frozen_stages > 0:\n",
    "            self._freeze_stages()\n",
    "        self.count_parameters()\n",
    "    @property\n",
    "    def norm1(self):\n",
    "        return self.ln1\n",
    "\n",
    "    @property\n",
    "    def norm2(self):\n",
    "        return self.ln2\n",
    "\n",
    "    def init_weights(self):\n",
    "        super(Mamba2DModel, self).init_weights()\n",
    "\n",
    "        if not (isinstance(self.init_cfg, dict)\n",
    "                and self.init_cfg['type'] == 'Pretrained'):\n",
    "            if self.pos_embed is not None:\n",
    "                trunc_normal_(self.pos_embed, std=0.02)\n",
    "        else:\n",
    "            ckpt = self.init_cfg['checkpoint']\n",
    "            ckpt = torch.load(ckpt,map_location='cpu')['state_dict']\n",
    "            ckpt = {k.replace('backbone.','',1):v for k,v in ckpt.items() if  k.startswith('backbone.') }\n",
    "            curr_dict = self.state_dict()\n",
    "            for k,v in ckpt.items():\n",
    "                if k in curr_dict and (v_new:=curr_dict[k]).shape != v.shape:\n",
    "                    if 'patch_embed' in k:\n",
    "                        # n c H W\n",
    "                        v_resized = torch.nn.functional.interpolate(v,v_new.shape[-2:])\n",
    "                        assert v_resized.shape == v_new.shape\n",
    "                        ckpt[k] = v_resized\n",
    "                    elif 'pos_embed' in k:\n",
    "                        b,old_len,dim = v.shape\n",
    "                        old_d = int(np.sqrt(old_len))\n",
    "                        new_len = v_new.shape[1]\n",
    "                        new_d = int(np.sqrt(new_len))\n",
    "                        v_resized = v.reshape(b,old_d,old_d,dim).permute(0,3,1,2)\n",
    "                        v_resized = torch.nn.functional.interpolate(v_resized,(new_d,new_d)).flatten(2).permute(0,2,1)\n",
    "                        assert v_resized.shape == v_new.shape\n",
    "                        ckpt[k] = v_resized\n",
    "                    else:\n",
    "                        print(k,v_new.shape,v.shape)\n",
    "            res = self.load_state_dict(ckpt,strict=False)\n",
    "            print('----------init-------------------')\n",
    "            print(res)\n",
    "\n",
    "    def _prepare_pos_embed(self, state_dict, prefix, *args, **kwargs):\n",
    "        name = prefix + 'pos_embed'\n",
    "        if name not in state_dict.keys():\n",
    "            return\n",
    "\n",
    "        ckpt_pos_embed_shape = state_dict[name].shape\n",
    "        if (not self.with_cls_token\n",
    "                and ckpt_pos_embed_shape[1] == self.pos_embed.shape[1] + 1):\n",
    "            # Remove cls token from state dict if it's not used.\n",
    "            state_dict[name] = state_dict[name][:, 1:]\n",
    "            ckpt_pos_embed_shape = state_dict[name].shape\n",
    "\n",
    "        if self.pos_embed.shape != ckpt_pos_embed_shape:\n",
    "            from mmengine.logging import MMLogger\n",
    "            logger = MMLogger.get_current_instance()\n",
    "            logger.info(\n",
    "                f'Resize the pos_embed shape from {ckpt_pos_embed_shape} '\n",
    "                f'to {self.pos_embed.shape}.')\n",
    "\n",
    "            ckpt_pos_embed_shape = to_2tuple(\n",
    "                int(np.sqrt(ckpt_pos_embed_shape[1] - self.num_extra_tokens)))\n",
    "            pos_embed_shape = self.patch_embed.init_out_size\n",
    "\n",
    "            state_dict[name] = resize_pos_embed(state_dict[name],\n",
    "                                                ckpt_pos_embed_shape,\n",
    "                                                pos_embed_shape,\n",
    "                                                self.interpolate_mode,\n",
    "                                                self.num_extra_tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_pos_embed(*args, **kwargs):\n",
    "        \"\"\"Interface for backward-compatibility.\"\"\"\n",
    "        return resize_pos_embed(*args, **kwargs)\n",
    "\n",
    "    def _freeze_stages(self):\n",
    "        # freeze position embedding\n",
    "        if self.pos_embed is not None:\n",
    "            self.pos_embed.requires_grad = False\n",
    "        # set dropout to eval model\n",
    "        self.drop_after_pos.eval()\n",
    "        # freeze patch embedding\n",
    "        self.patch_embed.eval()\n",
    "        for param in self.patch_embed.parameters():\n",
    "            param.requires_grad = False\n",
    "        # freeze pre-norm\n",
    "        for param in self.pre_norm.parameters():\n",
    "            param.requires_grad = False\n",
    "        # freeze cls_token\n",
    "        if self.cls_token is not None:\n",
    "            self.cls_token.requires_grad = False\n",
    "        # freeze layers\n",
    "        for i in range(1, self.frozen_stages + 1):\n",
    "            m = self.layers[i - 1]\n",
    "            m.eval()\n",
    "            for param in m.parameters():\n",
    "                param.requires_grad = False\n",
    "        # freeze the last layer norm\n",
    "        if self.frozen_stages == len(self.layers):\n",
    "            if self.final_norm:\n",
    "                self.ln1.eval()\n",
    "                for param in self.ln1.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            if self.out_type == 'avg_featmap':\n",
    "                self.ln2.eval()\n",
    "                for param in self.ln2.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x, patch_resolution = self.patch_embed(x)\n",
    "        x = x + resize_pos_embed(\n",
    "            self.pos_embed,\n",
    "            self.patch_resolution,\n",
    "            patch_resolution,\n",
    "            mode=self.interpolate_mode,\n",
    "            num_extra_tokens=self.num_extra_tokens)[:,self.num_extra_tokens:]\n",
    "        if self.is_2d:\n",
    "            assert self.cls_token is  None\n",
    "            x = rearrange(x,'n (h w) c-> n c h w',h=patch_resolution[0],w=patch_resolution[1])\n",
    "        if self.cls_token is not None:\n",
    "            # stole cls_tokens impl from Phil Wang, thanks\n",
    "            cls_token = self.cls_token.expand(B, -1, -1)\n",
    "            x = torch.cat((x,cls_token), dim=1) # append last\n",
    "        x = self.drop_after_pos(x)\n",
    "\n",
    "        x = self.pre_norm(x)\n",
    "        h = patch_resolution[0]\n",
    "        w = patch_resolution[1]\n",
    "        outs = []\n",
    "        residual = None\n",
    "        if self.update_interval:\n",
    "            raw_x = x\n",
    "            for i,layer in enumerate(self.layers):\n",
    "                z = i // 2\n",
    "                x_l,residual = layer(raw_x,residual,skip=False,h=h,w=w)\n",
    "                if layer.downsample:\n",
    "                        h,w = residual\n",
    "                        residual = None\n",
    "                x = x + x_l\n",
    "                if (i+1) % self.update_interval == 0 or i == len(self.layers) - 1:\n",
    "                    raw_x = x\n",
    "                #x = raw_x\n",
    "                if i == len(self.layers) - 1:\n",
    "                    x = (x + residual) if residual is not None else x\n",
    "                if i == len(self.layers) - 1 and self.final_norm:\n",
    "                    x = self.ln1(x)\n",
    "\n",
    "                if i in self.out_indices:\n",
    "                    outs.append(self._format_output(x, patch_resolution))\n",
    "        else:\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                if self.use_nd:\n",
    "                    x,residual = layer(x,residual,h=h,w=w)\n",
    "                    if layer.downsample:\n",
    "                        h,w = residual\n",
    "                        residual = None\n",
    "                else:\n",
    "                    x,residual = layer(x,residual,h=h,w=w)\n",
    "                    if layer.downsample:\n",
    "                        h,w = residual\n",
    "                        residual = None\n",
    "\n",
    "                if i == len(self.layers) - 1:\n",
    "                    x = (x + residual) if residual is not None else x\n",
    "                if i == len(self.layers) - 1 and self.final_norm:\n",
    "                    x = self.ln1(x)\n",
    "\n",
    "                if i in self.out_indices:\n",
    "                    outs.append(self._format_output(x, patch_resolution))\n",
    "\n",
    "        return tuple(outs)\n",
    "\n",
    "    def count_parameters(self,model=None):\n",
    "        if model is None:\n",
    "            model = self\n",
    "        table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "        total_params = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if not parameter.requires_grad:\n",
    "                continue\n",
    "            params = parameter.numel()\n",
    "            table.add_row([name, params])\n",
    "            total_params += params\n",
    "        self.total_parms = total_params\n",
    "        print(table)\n",
    "        print(f\"Total Trainable Params: {total_params}\")\n",
    "        return total_params\n",
    "    \n",
    "    def _format_output(self, x, hw):\n",
    "        if self.out_type == 'raw':\n",
    "            return x\n",
    "        if self.out_type == 'cls_token':\n",
    "            return x[:, -1]\n",
    "        if not self.is_2d:\n",
    "            patch_token = x[:, self.num_extra_tokens:]\n",
    "        else:\n",
    "            patch_token = x\n",
    "        if self.out_type == 'featmap':\n",
    "            B = x.size(0)\n",
    "            # (B, N, C) -> (B, H, W, C) -> (B, C, H, W)\n",
    "            if self.is_2d:\n",
    "                return patch_token\n",
    "            else:\n",
    "                return patch_token.reshape(B, *hw, -1).permute(0, 3, 1, 2)\n",
    "        if self.out_type == 'avg_featmap':\n",
    "            if self.is_2d:\n",
    "                return self.ln2(patch_token.flatten(2).mean(dim=-1))\n",
    "            else:\n",
    "                return self.ln2(patch_token.mean(dim=1))\n",
    "\n",
    "    def get_layer_depth(self, param_name: str, prefix: str = ''):\n",
    "        \"\"\"Get the layer-wise depth of a parameter.\n",
    "\n",
    "        Args:\n",
    "            param_name (str): The name of the parameter.\n",
    "            prefix (str): The prefix for the parameter.\n",
    "                Defaults to an empty string.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: The layer-wise depth and the num of layers.\n",
    "\n",
    "        Note:\n",
    "            The first depth is the stem module (``layer_depth=0``), and the\n",
    "            last depth is the subsequent module (``layer_depth=num_layers-1``)\n",
    "        \"\"\"\n",
    "        num_layers = self.num_layers + 2\n",
    "\n",
    "        if not param_name.startswith(prefix):\n",
    "            # For subsequent module like head\n",
    "            return num_layers - 1, num_layers\n",
    "\n",
    "        param_name = param_name[len(prefix):]\n",
    "\n",
    "        if param_name in ('cls_token', 'pos_embed'):\n",
    "            layer_depth = 0\n",
    "        elif param_name.startswith('patch_embed'):\n",
    "            layer_depth = 0\n",
    "        elif param_name.startswith('layers'):\n",
    "            layer_id = int(param_name.split('.')[1])\n",
    "            layer_depth = layer_id + 1\n",
    "        else:\n",
    "            layer_depth = num_layers - 1\n",
    "\n",
    "        return layer_depth, num_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92a04a4-743d-462e-95cd-3c9234ef7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"      ##\n",
    "##              CONFIG           ##   \n",
    "##    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"   ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee1790b-cea4-42a3-a448-6b4430467396",
   "metadata": {},
   "source": [
    "## TEST FOR TinyIMAGENET TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ffb6798-011f-4d10-ac5b-c957a678fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mmengine.hooks import Hook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "from mmengine.evaluator import BaseMetric\n",
    "from mmengine.registry import METRICS\n",
    "\n",
    "def train_step(self, data_batch, optim_wrapper):\n",
    "    # Forward pass\n",
    "    outputs = self(**data_batch)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()  # Change this to your loss function\n",
    "    loss = loss_fn(outputs, data_batch['labels'])\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    optim_wrapper.zero_grad()\n",
    "    loss.backward()\n",
    "    optim_wrapper.step()\n",
    "\n",
    "    # Collect predictions and loss\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'preds': outputs.detach()  # Ensure predictions are detached from the computation graph\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def val_step(self, data_batch):\n",
    "    # Forward pass\n",
    "    outputs = self(**data_batch)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()  # Change this to your loss function\n",
    "    loss = loss_fn(outputs, data_batch['labels'])\n",
    "\n",
    "    # Collect other metrics if needed\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'preds': outputs\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "# Ajout d'une metric pour le calcul de la loss de validation\n",
    "@METRICS.register_module()\n",
    "class LossValMetric(BaseMetric):\n",
    "    \"\"\"Loss evaluation metric.\"\"\"\n",
    "    default_prefix: Optional[str] = 'loss'\n",
    "\n",
    "    def __init__(self, collect_device: str = 'cpu', prefix: Optional[str] = None) -> None:\n",
    "        super().__init__(collect_device=collect_device, prefix=prefix)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()  # loss function \n",
    "\n",
    "    def process(self, data_batch: Sequence[Tuple[Any, Dict]], data_samples: Sequence[Dict]) -> None:\n",
    "        \"\"\"Process one batch of data samples and data_samples. The processed\n",
    "        results should be stored in ``self.results``, which will be used to\n",
    "        compute the metrics when all batches have been processed.\n",
    "        \"\"\"\n",
    "        outputs = [data_sample['pred_score'].to('cuda') for data_sample in data_samples]\n",
    "        labels = [data_sample['gt_label'].to('cuda') for data_sample in data_samples]\n",
    "   \n",
    "        # Stack outputs and labels to form tensors\n",
    "        outputs = torch.stack(outputs)\n",
    "        labels = torch.cat(labels)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_fn(outputs, labels).cpu().item()\n",
    "        self.results.append({'loss': loss})\n",
    "\n",
    "    def compute_metrics(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"Compute the metrics from processed results.\"\"\"\n",
    "        losses = [x['loss'] for x in results]\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        return {'loss': avg_loss}\n",
    "\n",
    "\n",
    "\n",
    "class MetricLoggerHook(Hook):\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_top1_acc = []\n",
    "        self.val_top1_acc = []\n",
    "        self.batch_losses = []  # To store batch losses for averaging\n",
    "        self.batch_top1_acc = []  # To store batch accuracies for averaging\n",
    "        self.all_labels = []  # To store true labels for confusion matrix\n",
    "        self.all_preds = []  # To store predicted labels for confusion matrix\n",
    "\n",
    "\n",
    "    def after_train_epoch(self, runner):\n",
    "        if self.batch_losses:\n",
    "            avg_loss = sum(self.batch_losses) / len(self.batch_losses)\n",
    "            self.train_loss.append(avg_loss)\n",
    "            self.batch_losses = []  # Reset batch losses for the next epoch\n",
    "        else:\n",
    "            self.train_loss.append(None)\n",
    "\n",
    "\n",
    "        if self.batch_top1_acc:\n",
    "            max_acc = max(self.batch_top1_acc)  # Get the maximum accuracy\n",
    "            self.train_top1_acc.append(max_acc)\n",
    "            self.batch_top1_acc = []  # Reset batch accuracies for the next epoch\n",
    "        else:\n",
    "            self.train_top1_acc.append(None)\n",
    "    \n",
    "\n",
    "    def after_val_epoch(self, runner, metrics=None):\n",
    "        if metrics and 'accuracy/top1' in metrics:\n",
    "            self.val_top1_acc.append(metrics['accuracy/top1'])\n",
    "        else:\n",
    "            self.val_top1_acc.append(None)\n",
    "\n",
    "        if metrics and 'loss/loss' in metrics:\n",
    "            self.val_loss.append(metrics['loss/loss'])\n",
    "        else:\n",
    "            self.val_loss.append(None)\n",
    "\n",
    "        self.save_to_csv('metrics.csv')\n",
    "            \n",
    "        self.plot_metrics()  # Plot metrics after each validation epoch\n",
    "        self.plot_confusion_matrix()  # Plot confusion matrix after each validation epoch\n",
    "        self.all_labels = []  # Reset labels for the next epoch\n",
    "        self.all_preds = []  # Reset predictions for the next epoch\n",
    "\n",
    "    def after_train_iter(self, runner, batch_idx, data_batch=None, outputs=None):\n",
    "        self.batch_losses.append(outputs['loss'].item())\n",
    "        runner.message_hub.update_scalar('loss', outputs['loss'].item())\n",
    "    \n",
    "        # Forward pass to get predictions\n",
    "        with torch.no_grad():\n",
    "            preds = runner.model(**data_batch)\n",
    "            preds = preds.argmax(dim=1)\n",
    "    \n",
    "        \n",
    "        # Assuming labels are in data_batch['data_samples']\n",
    "        labels = [sample.gt_label.item() for sample in data_batch['data_samples']]\n",
    "        labels = torch.tensor(labels).to(preds.device)  # Ensure labels are on the same device as preds\n",
    "        correct = preds.eq(labels).sum().item()\n",
    "        accuracy = correct / len(labels)\n",
    "        self.batch_top1_acc.append(accuracy)\n",
    "        runner.message_hub.update_scalar('top1_acc', accuracy)\n",
    "\n",
    "        \n",
    "    \n",
    "    def after_val_iter(self, runner, batch_idx, data_batch=None, outputs=None):\n",
    "        true_labels = [sample.gt_label.item() for sample in data_batch['data_samples']]\n",
    "        pred_labels = [output.pred_label.item() for output in outputs]\n",
    "\n",
    "        self.all_labels.extend(true_labels)\n",
    "        self.all_preds.extend(pred_labels)\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.train_loss) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Courbe de perte d'entraînement\n",
    "        plt.subplot(1, 2, 1)\n",
    "        if len(self.train_loss) > 0:\n",
    "            plt.plot(epochs, self.train_loss, label='Training loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss over Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "        # Courbe d'accuracy de validation\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if len(self.val_top1_acc) > 0:\n",
    "            plt.plot(epochs, self.val_top1_acc, label='Validation Top-1 Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Validation Accuracy over Epochs')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_confusion_matrix(self):\n",
    "        cm = confusion_matrix(self.all_labels, self.all_preds, normalize='true')\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(25, 25))  # Adjust the figure size as needed\n",
    "        disp.plot(ax=ax, cmap=plt.cm.Blues, xticks_rotation=90, values_format='.1f')\n",
    "        \n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        \n",
    "        # Ajuster la taille de la police\n",
    "        plt.xticks(fontsize=10) \n",
    "        plt.yticks(fontsize=10)  \n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def save_to_csv(self, filename):\n",
    "        epochs = range(1, len(self.train_loss) + 1)\n",
    "\n",
    "        #print(f\"epoch : {epochs}, train_loss : {self.train_loss}, val_acc : {self.val_top1_acc}\")\n",
    "        data = {\n",
    "            'Epoch': epochs,\n",
    "            'Training Loss': self.train_loss,\n",
    "            'Validation Loss': self.val_loss,\n",
    "            'Training Top-1 Accuracy': self.train_top1_acc,\n",
    "            'Validation Top-1 Accuracy': self.val_top1_acc\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2956c131-5e2d-4fae-96ad-82bac3547d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch\n",
    "from mmengine.config import Config, ConfigDict\n",
    "from mmengine.registry import RUNNERS\n",
    "from mmengine.runner import Runner\n",
    "import logging\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DataSample:\n",
    "  def __init__(self, gt_label):\n",
    "   self.gt_label = torch.tensor([gt_label]).to('cuda')\n",
    "\n",
    "  def set_pred_score(self, score):\n",
    "   self.pred_score = score\n",
    "   return self\n",
    "\n",
    "  def set_pred_label(self, label):\n",
    "   self.pred_label = label\n",
    "   return self\n",
    "\n",
    "  @property\n",
    "  def gt_score(self):\n",
    "   # Cette méthode peut être modifiée selon vos besoins spécifiques\n",
    "   return None\n",
    "\n",
    "  def __contains__(self, key):\n",
    "   return key in self.__dict__\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "   return self.__dict__[key]\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, dataset):\n",
    "   self.dataset = dataset\n",
    "\n",
    "  def __len__(self):\n",
    "   return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "   data = self.dataset[idx]\n",
    "   return {'inputs': data[0], 'labels': data[1]}\n",
    "      \n",
    "\n",
    "# Fonction custom_collate_fn\n",
    "def custom_collate_fn(batch):\n",
    "    inputs = torch.stack([item['inputs'] for item in batch]).to('cuda')\n",
    "    labels = torch.tensor([item['labels'] for item in batch]).to('cuda')\n",
    "    data_samples = [DataSample(label.item()) for label in labels]\n",
    "    return {'inputs': inputs, 'data_samples': data_samples}\n",
    "\n",
    "# Fonction pour lire les annotations de validation\n",
    "def read_val_annotations(val_dir):\n",
    "    val_annotations_path = os.path.join(val_dir, 'val_annotations.txt')\n",
    "    with open(val_annotations_path, 'r') as f:\n",
    "        val_annotations = f.readlines()\n",
    "    \n",
    "    val_labels = {}\n",
    "    for line in val_annotations:\n",
    "        parts = line.split('\\t')\n",
    "        val_labels[parts[0]] = parts[1]\n",
    "    \n",
    "    return val_labels\n",
    "\n",
    "# Fonction pour créer le dataset de validation\n",
    "class TinyImageNetValidationDataset(Dataset):\n",
    "    def __init__(self, val_dir, transform=None):\n",
    "        self.val_dir = val_dir\n",
    "        self.transform = transform\n",
    "        self.val_labels = read_val_annotations(val_dir)\n",
    "        self.img_dir = os.path.join(val_dir, 'images')\n",
    "        self.img_names = list(self.val_labels.keys())\n",
    "        \n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(sorted(set(self.val_labels.values())))}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        label = self.class_to_idx[self.val_labels[img_name]]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "# Define the configuration dictionary directly in the notebook\n",
    "cfg = ConfigDict(\n",
    "  default_scope='mmpretrain',\n",
    "  env_cfg=dict(\n",
    "   cudnn_benchmark=False,\n",
    "   mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
    "   dist_cfg=dict(backend='nccl'),\n",
    "  ),\n",
    "  vis_backends=[dict(type='LocalVisBackend')],\n",
    "  visualizer=dict(type='UniversalVisualizer', vis_backends=[\n",
    "               dict(type='LocalVisBackend')]),\n",
    "  log_level='INFO',\n",
    "  load_from=None, #'/home/simon/Documents/MambaND/MambaNDoff/image_classification/work_dirs/cifar10_experiment/best_accuracy_top1_epoch_30.pth',\n",
    "  resume=False,\n",
    "  randomness=dict(seed=77, diff_rank_seed=True),\n",
    "  train_cfg=dict(by_epoch=True, max_epochs=60),\n",
    "  val_cfg=dict(),\n",
    "  test_cfg=dict(),\n",
    "  auto_scale_lr=dict(base_batch_size=32),\n",
    "  model=dict(\n",
    "   type='ImageClassifier',\n",
    "   backbone=dict(\n",
    "       type='Mamba2DModel',\n",
    "       arch='small',\n",
    "       img_size=64,\n",
    "       patch_size=8,\n",
    "       out_type='avg_featmap',\n",
    "       drop_path_rate=0.1,\n",
    "       drop_rate=0.1,\n",
    "       with_cls_token=False,\n",
    "       final_norm=True,\n",
    "       fused_add_norm=False,\n",
    "       d_state=16,\n",
    "       is_2d=False,\n",
    "       use_v2=False,\n",
    "       constant_dim=True,\n",
    "       downsample=(9,),\n",
    "       force_a2=False,\n",
    "       use_mlp=False,\n",
    "   ),\n",
    "   neck=None,\n",
    "   head=dict(\n",
    "       type='LinearClsHead',\n",
    "       num_classes=200,\n",
    "       in_channels=384,\n",
    "       loss=dict(\n",
    "           type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),\n",
    "       init_cfg=[dict(type='TruncNormal', layer='Linear', std=2e-5)]),\n",
    "   train_cfg=dict(augments=[\n",
    "       dict(type='Mixup', alpha=0.8),\n",
    "       dict(type='CutMix', alpha=1.0)\n",
    "   ])),\n",
    "  optim_wrapper=dict(\n",
    "   optimizer=dict(\n",
    "       type='AdamW', lr=1e-3, weight_decay=0.1, betas=(0.9, 0.999)),\n",
    "   constructor='LearningRateDecayOptimWrapperConstructor',\n",
    "   clip_grad=dict(max_norm=2.0),\n",
    "   paramwise_cfg=dict(\n",
    "       norm_decay_mult=0.1,\n",
    "       layer_decay_rate=0.95,\n",
    "       custom_keys={\n",
    "           '.ln': dict(decay_mult=0.0),\n",
    "           '.bias': dict(decay_mult=0.0),\n",
    "           '.cls_token': dict(decay_mult=0.0),\n",
    "           '.pos_embed': dict(decay_mult=0.0),\n",
    "           '.A_log': dict(decay_mult=0.1),\n",
    "           '.A2_log': dict(decay_mult=0.1),\n",
    "           '.absolute_pos_embed': dict(decay_mult=0.0),\n",
    "       })),\n",
    "  param_scheduler=[\n",
    "   dict(\n",
    "     type='LinearLR',\n",
    "     start_factor=1e-4,\n",
    "     by_epoch=True,\n",
    "     begin=0,\n",
    "     end=10,\n",
    "     convert_to_iter_based=True),\n",
    "   dict(\n",
    "       type='CosineAnnealingLR',\n",
    "       by_epoch=True,\n",
    "       begin=10,\n",
    "       eta_min=1e-5,\n",
    "       convert_to_iter_based=True)\n",
    "  ],\n",
    "  default_hooks=dict(\n",
    "   timer=dict(type='IterTimerHook'),\n",
    "   logger=dict(type='LoggerHook', interval=100),\n",
    "   param_scheduler=dict(type='ParamSchedulerHook'),\n",
    "   checkpoint=dict(type='CheckpointHook', interval=1, max_keep_ckpts=3, save_best='auto'),\n",
    "   sampler_seed=dict(type='DistSamplerSeedHook'),\n",
    "   visualization=dict(type='VisualizationHook', enable=True),\n",
    "  ),\n",
    "  data_preprocessor=dict(\n",
    "   num_classes=200,\n",
    "   mean=[0.5, 0.5, 0.5],\n",
    "   std=[0.5, 0.5, 0.5],\n",
    "   to_rgb=True,\n",
    "  )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Chemins pour les données tiny Imagenet\n",
    "data_root = './'\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cfg.data_preprocessor.mean,cfg.data_preprocessor.std),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.03, 0.03))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[x / 255.0 for x in cfg.data_preprocessor['mean']],\n",
    "                     std=[x / 255.0 for x in cfg.data_preprocessor['std']])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(data_root, 'train'), transform=transform_train)\n",
    "val_dataset = TinyImageNetValidationDataset(val_dir=os.path.join(data_root, 'val'), transform=transform_test)\n",
    "\n",
    "# DataLoader pour l'entraînement\n",
    "train_dataloader = DataLoader(\n",
    "    CustomDataset(train_dataset),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# DataLoader pour la validation\n",
    "val_dataloader = DataLoader(\n",
    "    CustomDataset(val_dataset),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# Évaluateur\n",
    "cfg.val_evaluator = [dict(type='Accuracy', topk=(1, 5)), dict(type='LossValMetric')]\n",
    "cfg.test_evaluator = cfg.val_evaluator\n",
    "\n",
    "# Affichage des configurations\n",
    "print(\"Train dataloader:\", train_dataloader)\n",
    "print(\"Validation dataloader:\", val_dataloader)\n",
    "print(\"Validation evaluator:\", cfg.val_evaluator)\n",
    "\n",
    "# ####################   DEBOGAGE VISUALISATION DONNEE   ###############################\n",
    "\n",
    "# def visualize_samples(dataset):\n",
    "#     for i in range(0,5):\n",
    "#         img, label = dataset[i]\n",
    "#         plt.imshow(img.permute(1, 2, 0))  # Change the format for displaying\n",
    "#         plt.title(f'Label: {label}')\n",
    "#         plt.show()\n",
    "\n",
    "\n",
    "# # Visualiser quelques échantillons du dataset\n",
    "# print(\"Visualisation des échantillons de l'ensemble d'entraînement :\")\n",
    "# visualize_samples(train_dataset)\n",
    "\n",
    "# print(\"Visualisation des échantillons de l'ensemble de validation :\")\n",
    "# visualize_samples(val_dataset)\n",
    "# ####################################################################################\n",
    "\n",
    "# Simuler les arguments d'entrée\n",
    "class Args:\n",
    "    work_dir = None\n",
    "    resume = None\n",
    "    amp = False\n",
    "    no_validate = False\n",
    "    auto_scale_lr = False\n",
    "    no_pin_memory = False\n",
    "    no_persistent_workers = False\n",
    "    cfg_options = None\n",
    "    launcher = 'none'\n",
    "    local_rank = 0\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def merge_args(cfg, args):\n",
    "    \"\"\"Merge CLI arguments to config.\"\"\"\n",
    "    if args.no_validate:\n",
    "        cfg.val_cfg = None\n",
    "        cfg.val_dataloader = None\n",
    "        cfg.val_evaluator = None\n",
    "\n",
    "    cfg.launcher = args.launcher\n",
    "\n",
    "    if args.work_dir is not None:\n",
    "        cfg.work_dir = args.work_dir\n",
    "    elif cfg.get('work_dir', None) is None:\n",
    "        cfg.work_dir = os.path.join('./work_dirs', 'tinyimagenet_experiment')\n",
    "\n",
    "    if args.amp:\n",
    "        cfg.optim_wrapper.type = 'AmpOptimWrapper'\n",
    "        cfg.optim_wrapper.setdefault('loss_scale', 'dynamic')\n",
    "\n",
    "    if args.resume == 'auto':\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = None\n",
    "    elif args.resume is not None:\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = args.resume\n",
    "\n",
    "    if args.auto_scale_lr:\n",
    "        cfg.auto_scale_lr.enable = True\n",
    "\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "# Merging args with the configuration\n",
    "cfg = merge_args(cfg, args)\n",
    "\n",
    "# Initialisation du hook personnalisé\n",
    "metric_logger_hook = MetricLoggerHook()\n",
    "\n",
    "# Initialize runner and start training\n",
    "runner = Runner(\n",
    "    model=cfg.model,\n",
    "    work_dir=cfg.work_dir,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    test_dataloader=val_dataloader,  # Assuming test and val dataloaders are the same\n",
    "    train_cfg=cfg.train_cfg,\n",
    "    val_cfg=cfg.val_cfg,\n",
    "    test_cfg=cfg.test_cfg,\n",
    "    auto_scale_lr=cfg.auto_scale_lr,\n",
    "    optim_wrapper=cfg.optim_wrapper,\n",
    "    param_scheduler=cfg.param_scheduler,\n",
    "    val_evaluator=cfg.val_evaluator,\n",
    "    test_evaluator=cfg.test_evaluator,\n",
    "    default_hooks=cfg.default_hooks,\n",
    "    custom_hooks=[metric_logger_hook],\n",
    "    data_preprocessor=cfg.data_preprocessor,\n",
    "    load_from=cfg.load_from,\n",
    "    resume=cfg.resume,\n",
    "    launcher=cfg.launcher,\n",
    "    env_cfg=cfg.env_cfg,\n",
    "    log_level=cfg.log_level,\n",
    "    visualizer=cfg.visualizer,\n",
    "    default_scope=cfg.default_scope,\n",
    "    randomness=cfg.randomness,\n",
    "    cfg=cfg\n",
    ")\n",
    "\n",
    "# Déplacer le modèle et les autres composants sur l'appareil approprié\n",
    "runner.model.to('cuda')\n",
    "\n",
    "# Start training\n",
    "logger.info(\"Starting training...\")\n",
    "# Sample batch display for debugging\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(\"Sample batch inputs shape:\", sample_batch['inputs'].shape)\n",
    "print(\"Sample batch labels shape:\", sample_batch['data_samples'][0].gt_label.shape)\n",
    "\n",
    "runner.train()\n",
    "\n",
    "# Après l'entraînement, tracer les courbes\n",
    "metric_logger_hook.plot_metrics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mamba_ND",
   "language": "python",
   "name": "mamband"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
